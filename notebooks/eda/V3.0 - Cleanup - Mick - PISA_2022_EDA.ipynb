{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OruSNgUE4iRt"
   },
   "source": [
    "# EDA Notebook V3 - With merged dataset\n",
    "## Notes - Feb 16th 08:00 AM PST\n",
    "\n",
    "This notebook uses a **merged version** of the SPSS file. The follwing changes were made:\n",
    "\n",
    "From Selene : \n",
    "\n",
    "I did some more [data cleaning](https://7z4vtvpqcoxouiu.studio.us-west-2.sagemaker.aws/jupyterlab/default/lab/tree/RTC%3AData_merging.ipynb) (this is Selenes Data Cleaning Notebook moved to our Server) for the student dataset and school dataset\n",
    "\n",
    "- Recoded all valid skip / not applicable / invalid / no response to NaN\n",
    "- Looked at the codebook for each variable and checked if there were any other values that should be recoded as NaN (or another value)\n",
    "- One-hot encoded all the categorical variables\n",
    "- Deleted students from 4 groups that are not nationally representative\n",
    "- Changed the country names to full country names (instead of 3 letter codes)\n",
    "- Merged the student dataset and school dataset\n",
    "\n",
    "Note that I didn't merge in the Teacher dataset because there is no way to match each student with a teacher. (We know which school each student went to, but if there are multiple teachers in that school, we can't match each student to a teacher.)\n",
    "\n",
    "- More details are in the following folder: https://drive.google.com/drive/u/0/folders/1LsSvVuqH5oWC4f8c59fj1taEvATuvdoj\n",
    "- Details on data cleaning steps: \"Data cleaning steps\"\n",
    "- Python notebook I used for data cleaning:  \"Data cleaning steps.ipynb\" (I did everything in Colab)\n",
    "- List of variables in the final cleaned dataset\n",
    "- Final cleaned dataset:  \"PISA_cleaned_dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2j7ZleWu3y3B"
   },
   "source": [
    "# File Descriptions\n",
    "\n",
    "These files\n",
    "\n",
    " - **pisa-2022-182var-recoded-02102025.csv_local.csv**\n",
    "    - This is Mick's new main data file with the MATH_Proficient variable and 180 other variables\n",
    " - **PISA 2022 - CSV - DD - Sheet1.csv**\n",
    "    - This is a data dictionary of the fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick Specific Columns or Read the Whole File?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_read_full = [\n",
    "    \"MATH_Proficient\", \"CNT\", \"CNTRYID\", \"CNTSCHID\", \"CNTSTUID\", \"SISCO\", \"ST347Q01JA\", \"ST347Q02JA\",\n",
    "    \"ST349Q01JA\", \"ST350Q01JA\", \"ST356Q01JA\", \"ST322Q01JA\", \"ST322Q02JA\", \"ST322Q03JA\", \"ST322Q04JA\",\n",
    "    \"ST322Q06JA\", \"ST322Q07JA\", \"DURECEC\", \"EFFORT1\", \"EFFORT2\", \"ST259Q01JA\", \"WB164Q01HA\", \"HOMEPOS\",\n",
    "    \"ST004D01T\", \"GRADE\", \"REPEAT\", \"EXPECEDU\", \"ICTAVSCH\", \"ICTAVHOM\", \"ICTDISTR\", \"IMMIG\", \"LANGN\",\n",
    "    \"TARDYSD\", \"ST226Q01JA\", \"ST016Q01NA\", \"MISSSC\", \"Option_UH\", \"OECD\", \"PAREDINT\", \"BMMJ1\", \"BFMJ2\",\n",
    "    \"WB163Q06HA\", \"WB163Q07HA\", \"ST230Q01JA\", \"SKIPPING\", \"IC180Q01JA\", \"IC180Q08JA\", \"ST059Q02JA\",\n",
    "    \"ST296Q04JA\", \"WB176Q01HA\", \"STUDYHMW\", \"IC184Q01JA\", \"IC184Q02JA\", \"IC184Q03JA\", \"IC184Q04JA\",\n",
    "    \"ST059Q01TA\", \"ST296Q01JA\", \"ST272Q01JA\", \"ST268Q01JA\", \"ST268Q04JA\", \"ST268Q07JA\", \"ST293Q04JA\",\n",
    "    \"ST297Q01JA\", \"ST297Q03JA\", \"ST297Q05JA\", \"ST297Q06JA\", \"ST297Q07JA\", \"ST297Q09JA\", \"WB165Q01HA\",\n",
    "    \"WB166Q01HA\", \"WB166Q02HA\", \"WB166Q03HA\", \"WB166Q04HA\", \"ST258Q01JA\", \"ST294Q01JA\", \"ST295Q01JA\",\n",
    "    \"WB150Q01HA\", \"WB156Q01HA\", \"WB158Q01HA\", \"WB160Q01HA\", \"WB161Q01HA\", \"WB171Q01HA\", \"WB171Q02HA\",\n",
    "    \"WB171Q03HA\", \"WB171Q04HA\", \"WB172Q01HA\", \"WB173Q01HA\", \"WB173Q02HA\", \"WB173Q03HA\", \"WB173Q04HA\",\n",
    "    \"WB177Q01HA\", \"WB177Q02HA\", \"WB177Q03HA\", \"WB177Q04HA\", \"WB032Q01NA\", \"WB032Q02NA\", \"WB031Q01NA\",\n",
    "    \"EXERPRAC\", \"STUBMI\", \"RELATST\", \"BELONG\", \"BULLIED\", \"FEELSAFE\", \"SCHRISK\", \"PERSEVAGR\", \"CURIOAGR\",\n",
    "    \"COOPAGR\", \"EMPATAGR\", \"ASSERAGR\", \"STRESAGR\", \"EMOCOAGR\", \"GROSAGR\", \"INFOSEEK\", \"FAMSUP\", \"DISCLIM\",\n",
    "    \"TEACHSUP\", \"COGACRCO\", \"COGACMCO\", \"EXPOFA\", \"EXPO21ST\", \"MATHEFF\", \"MATHEF21\", \"FAMCON\", \"ANXMAT\",\n",
    "    \"MATHPERS\", \"CREATEFF\", \"CREATSCH\", \"CREATFAM\", \"CREATAS\", \"CREATOOS\", \"CREATOP\", \"OPENART\", \"IMAGINE\",\n",
    "    \"SCHSUST\", \"LEARRES\", \"PROBSELF\", \"FAMSUPSL\", \"FEELLAH\", \"SDLEFF\", \"ICTRES\", \"ESCS\", \"FLSCHOOL\",\n",
    "    \"FLMULTSB\", \"FLFAMILY\", \"ACCESSFP\", \"FLCONFIN\", \"FLCONICT\", \"ACCESSFA\", \"ATTCONFM\", \"FRINFLFM\",\n",
    "    \"ICTSCH\", \"ICTHOME\", \"ICTQUAL\", \"ICTSUBJ\", \"ICTENQ\", \"ICTFEED\", \"ICTOUT\", \"ICTWKDY\", \"ICTWKEND\",\n",
    "    \"ICTREG\", \"ICTINFO\", \"ICTEFFIC\", \"BODYIMA\", \"SOCONPA\", \"LIFESAT\", \"PSYCHSYM\", \"SOCCON\", \"EXPWB\",\n",
    "    \"CURSUPP\", \"PQMIMP\", \"PQMCAR\", \"PARINVOL\", \"PQSCHOOL\", \"PASCHPOL\", \"ATTIMMP\", \"CREATHME\", \"CREATACT\",\n",
    "    \"CREATOPN\", \"CREATOR\", \"WORKPAY\", \"WORKHOME\"\n",
    "]\n",
    "\n",
    "columns_to_read_test = [\n",
    "    \"MATH_Proficient\", \"CNT\", \"CNTRYID\", \"CNTSCHID\", \"CNTSTUID\", \"SISCO\", \"WB031Q01NA\"\n",
    "]\n",
    "\n",
    "columns_to_read_just_wle = [\n",
    "    \"MATH_Proficient\", \"CNT\", \"CNTRYID\", \"CNTSCHID\", \"CNTSTUID\", \"SISCO\",\n",
    "    \"EXERPRAC\", \"STUBMI\", \"RELATST\", \"BELONG\", \"BULLIED\", \"FEELSAFE\", \"SCHRISK\", \"PERSEVAGR\", \"CURIOAGR\",\n",
    "    \"COOPAGR\", \"EMPATAGR\", \"ASSERAGR\", \"STRESAGR\", \"EMOCOAGR\", \"GROSAGR\", \"INFOSEEK\", \"FAMSUP\", \"DISCLIM\",\n",
    "    \"TEACHSUP\", \"COGACRCO\", \"COGACMCO\", \"EXPOFA\", \"EXPO21ST\", \"MATHEFF\", \"MATHEF21\", \"FAMCON\", \"ANXMAT\",\n",
    "    \"MATHPERS\", \"CREATEFF\", \"CREATSCH\", \"CREATFAM\", \"CREATAS\", \"CREATOOS\", \"CREATOP\", \"OPENART\", \"IMAGINE\",\n",
    "    \"SCHSUST\", \"LEARRES\", \"PROBSELF\", \"FAMSUPSL\", \"FEELLAH\", \"SDLEFF\", \"ICTRES\", \"ESCS\", \"FLSCHOOL\",\n",
    "    \"FLMULTSB\", \"FLFAMILY\", \"ACCESSFP\", \"FLCONFIN\", \"FLCONICT\", \"ACCESSFA\", \"ATTCONFM\", \"FRINFLFM\",\n",
    "    \"ICTSCH\", \"ICTHOME\", \"ICTQUAL\", \"ICTSUBJ\", \"ICTENQ\", \"ICTFEED\", \"ICTOUT\", \"ICTWKDY\", \"ICTWKEND\",\n",
    "    \"ICTREG\", \"ICTINFO\", \"ICTEFFIC\", \"BODYIMA\", \"SOCONPA\", \"LIFESAT\", \"PSYCHSYM\", \"SOCCON\", \"EXPWB\",\n",
    "    \"CURSUPP\", \"PQMIMP\", \"PQMCAR\", \"PARINVOL\", \"PQSCHOOL\", \"PASCHPOL\", \"ATTIMMP\", \"CREATHME\", \"CREATACT\",\n",
    "    \"CREATOPN\", \"CREATOR\", \"WORKPAY\", \"WORKHOME\"\n",
    "]\n",
    "\n",
    "# Read specific columns\n",
    "columns_to_read = columns_to_read_full\n",
    "\n",
    "#Passing usecols=None explicitly tells Pandas to read all columns.\n",
    "columns_to_read = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i0I8sRiV2dx6",
    "outputId": "12cd28ea-37b9-4762-f5f2-514addc09ebe"
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define local file path\n",
    "local_file_path = \"PISA_cleaned_dataset.csv\"  # Change as needed\n",
    "\n",
    "# Define S3 details\n",
    "bucket_name = \"sagemaker-us-west-2-986030204467\"\n",
    "file_key = \"capstone/testfiles/PISA_cleaned_dataset.csv\"\n",
    "\n",
    "# AWS credentials are usually stored in ~/.aws/credentials or IAM roles (if running on AWS services)\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "# Read the file from S3\n",
    "response = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "\n",
    "\n",
    "# Check if the file exists locally\n",
    "if os.path.exists(local_file_path):\n",
    "    print(\"üìÇ Loading data from local file...\")\n",
    "    df = pd.read_csv(local_file_path, usecols=columns_to_read)\n",
    "else:\n",
    "    print(\"‚òÅÔ∏è Downloading data from S3...\")\n",
    "    \n",
    "    # Create S3 client\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "\n",
    "    # Download the file from S3\n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "\n",
    "    # Read the file into pandas DataFrame\n",
    "    df = pd.read_csv(response[\"Body\"], usecols=columns_to_read)\n",
    "\n",
    "    # Save a local copy for future use\n",
    "    df.to_csv(local_file_path, index=False)\n",
    "    print(f\"‚úÖ File saved locally as {local_file_path}\")\n",
    "\n",
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tHurFIG3uMiy",
    "outputId": "4914acbe-8f23-4c64-abd5-32b6c482479f"
   },
   "outputs": [],
   "source": [
    "# Load the metadata file\n",
    "file_key = \"capstone/testfiles/PISA 2022 - CSV - DD - Sheet1.csv\"  # e.g., \"datasets/mydata.csv\"\n",
    "\n",
    "# Read the file from S3\n",
    "response = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "\n",
    "df_metadata = pd.read_csv(response[\"Body\"], usecols=[0, 4], names=[\"Field_ID\", \"Field_Description\"], header=0)\n",
    "\n",
    "# Display first few rows to verify\n",
    "print(df_metadata.head())\n",
    "\n",
    "# Convert to dictionary for quick lookup\n",
    "field_description_lookup = df_metadata.set_index(\"Field_ID\")[\"Field_Description\"].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Invalid Value Rate Before Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Define invalid values with their descriptions\n",
    "invalid_values = {95: \"Skipped\", 97: \"N/A\", 98: \"Invalid\", 99: \"No Response\"}\n",
    "\n",
    "# Calculate the percentage of each invalid value per column\n",
    "invalid_counts = {desc: (df == val).mean() * 100 for val, desc in invalid_values.items()}\n",
    "\n",
    "# Convert to DataFrame for plotting\n",
    "invalid_df = pd.DataFrame(invalid_counts)\n",
    "\n",
    "# Filter out columns where all invalid percentages are below 0.1%\n",
    "invalid_df = invalid_df.loc[(invalid_df > 0.1).any(axis=1)]\n",
    "\n",
    "# Proceed only if filtered data is not empty\n",
    "if not invalid_df.empty:\n",
    "    # Sort by total invalid percentage (descending)\n",
    "    invalid_df[\"Total Invalid %\"] = invalid_df.sum(axis=1)\n",
    "    invalid_df = invalid_df.sort_values(by=\"Total Invalid %\", ascending=False)\n",
    "\n",
    "    # Plot stacked bar chart\n",
    "    invalid_df.drop(columns=[\"Total Invalid %\"]).plot(kind='bar', stacked=True, figsize=(15, 6), colormap=\"tab10\", alpha=0.85)\n",
    "\n",
    "    # Formatting\n",
    "    plt.xlabel(\"Variables\", fontsize=12)\n",
    "    plt.ylabel(\"Percentage of Invalid Responses\", fontsize=12)\n",
    "    plt.title(\"Percentage of Invalid Responses per Variable (>0.1%)\", fontsize=14)\n",
    "    plt.xticks(rotation=90, fontsize=8)\n",
    "    plt.legend(title=\"Invalid Response Type\", bbox_to_anchor=(1, 1), loc=\"upper left\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No variables have invalid values above 0.1%.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Check Before Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique values including NaNs\n",
    "value_counts_table = df['WB031Q01NA'].value_counts(ascending=True, dropna=False).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "value_counts_table.columns = ['Value', 'Count']\n",
    "\n",
    "print(\"üìä Sorted Value Counts in 'WB031Q01NA':\")\n",
    "print(value_counts_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Invalid Values\n",
    "\n",
    "This cleans the following invalid values from the dataset and turns them to nulls\n",
    "\n",
    "## Exact matches of numbers on ANY column:\n",
    "\n",
    "- 95 or 95.0000 or 995.00 or 9995.00 or 9999995.0000 (Valid Skip)\n",
    "- 97 or 97.0000 or 997.00 or 9997.00 or 9999997.0000 (Not Applicable)\n",
    "- 98 or 98.0000 or 998.00 or 9998.00 or 9999998.0000 (Invalid)\n",
    "- 99 or 99.0000 or 999.00 or 9999.00 or 9999999.0000 (No Response)\n",
    "- -999 (System Missing recoded to a negative number)\n",
    "\n",
    "## Column specific matches\n",
    "\n",
    "- SISCO, recode 5,7,8,9 to NaN\n",
    "- ST004D01T, recode 5,7,8,9 to NaN\n",
    "- REPEAT, recode 5,7,8,9 to NaN\n",
    "- ST322Q01JA, recode 6 to NaN\n",
    "- ST322Q02JA, recode 6 to NaN\n",
    "- ST322Q03JA, recode 6 to NaN\n",
    "- ST322Q04JA, recode 6 to NaN\n",
    "- ST322Q06JA, recode 6 to NaN\n",
    "- ST322Q07JA, recode 6 to NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cleaning function\n",
    "def clean_dataset(df):\n",
    "    \"\"\"\n",
    "    Cleans invalid values in the dataset by converting specified values to NaN.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input dataset to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The cleaned dataset.\n",
    "    \"\"\"\n",
    "    # Define general invalid values to be replaced with NaN\n",
    "    invalid_values = {\n",
    "        95, 95.0000, 995.00, 9995.00, 9999995.0000,  # Valid Skip\n",
    "        97, 97.0000, 997.00, 9997.00, 9999997.0000,  # Not Applicable\n",
    "        98, 98.0000, 998.00, 9998.00, 9999998.0000,  # Invalid\n",
    "        99, 99.0000, 999.00, 9999.00, 9999999.0000,  # No Response\n",
    "        -999  # System Missing recoded to a negative number\n",
    "    }\n",
    "\n",
    "    # Convert all numeric columns to float (if they are not already)\n",
    "    for col in df.select_dtypes(include=['int64', 'float64']).columns:\n",
    "        df[col] = df[col].astype(float)  # Ensures comparisons work properly\n",
    "\n",
    "    # Apply general invalid value replacement across all columns\n",
    "    df = df.replace(invalid_values, np.nan)\n",
    "\n",
    "    # Define column-specific recoding\n",
    "    column_specific_invalids = {\n",
    "        \"SISCO\": [5, 7, 8, 9],\n",
    "        \"ST004D01T\": [5, 7, 8, 9],\n",
    "        \"REPEAT\": [5, 7, 8, 9],\n",
    "        \"ST322Q01JA\": [6],\n",
    "        \"ST322Q02JA\": [6],\n",
    "        \"ST322Q03JA\": [6],\n",
    "        \"ST322Q04JA\": [6],\n",
    "        \"ST322Q06JA\": [6],\n",
    "        \"ST322Q07JA\": [6]\n",
    "    }\n",
    "\n",
    "    # Apply column-specific replacements using dictionary mapping\n",
    "    for col, values in column_specific_invalids.items():\n",
    "        if col in df.columns:  # Only process if the column exists\n",
    "            df[col] = df[col].replace({v: np.nan for v in values})\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the cleaning function to the dataset\n",
    "df = clean_dataset(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Check After Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique values including NaNs\n",
    "value_counts_table = df['WB031Q01NA'].value_counts(ascending=True, dropna=False).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "value_counts_table.columns = ['Value', 'Count']\n",
    "\n",
    "print(\"üìä Sorted Value Counts in 'WB031Q01NA':\")\n",
    "print(value_counts_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "id": "RPjdN5tu7y3I",
    "outputId": "09a90b19-5207-4a10-d6ea-70729df87e09"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "# Sample DataFrame (replace with your actual DataFrame)\n",
    "# df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Calculate percentage of missing values\n",
    "null_percentages = df.isnull().mean() * 100\n",
    "\n",
    "# Define an interactive function\n",
    "def plot_missing(threshold=20):\n",
    "    filtered = null_percentages[(null_percentages >= threshold) & (null_percentages > 0)]\n",
    "    filtered = filtered.sort_values(ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(15, 6))\n",
    "    filtered.plot(kind='bar', color='tomato', alpha=0.75)\n",
    "    plt.xlabel(\"Variables\", fontsize=12)\n",
    "    plt.ylabel(\"Percentage of Missing Values\", fontsize=12)\n",
    "    plt.title(f\"Missing Values per Variable (> {threshold}%)\", fontsize=14)\n",
    "    plt.xticks(rotation=90, fontsize=8)\n",
    "    plt.show()\n",
    "\n",
    "# Create an interactive slider\n",
    "interact(plot_missing, threshold=widgets.FloatSlider(min=0, max=100, step=0.5, value=20, description=\"Threshold\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude 'CNT' column from the missing values calculation\n",
    "missing_by_country = df.groupby(\"CNT\").apply(lambda x: x.drop(columns=[\"CNT\"]).isnull().mean() * 100)\n",
    "\n",
    "# No need to droplevel since CNT is no longer included in the index\n",
    "missing_by_country = missing_by_country.T  # Transpose for readability\n",
    "\n",
    "# Save to CSV\n",
    "csv_filename = \"v3.0-cleanup-mick-missing_values_by_country_v2.csv\"\n",
    "missing_by_country.to_csv(csv_filename, index=True)\n",
    "\n",
    "print(f\"‚úÖ Missing values breakdown saved to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(18, 10))  # Increase figure size for better readability\n",
    "\n",
    "ax = sns.heatmap(\n",
    "    missing_by_country, \n",
    "    cmap=\"Reds\", \n",
    "    annot=False, \n",
    "    linewidths=0.5, \n",
    "    cbar=True, \n",
    "    vmin=0, vmax=100\n",
    ")\n",
    "\n",
    "# Ensure all variables (y-axis) are displayed\n",
    "ax.set_yticks(range(len(missing_by_country.index)))  # Set tick positions\n",
    "ax.set_yticklabels(missing_by_country.index, fontsize=8)  # Adjust font size\n",
    "\n",
    "# Ensure all countries (x-axis) are displayed\n",
    "ax.set_xticks(range(len(missing_by_country.columns)))  # Set tick positions\n",
    "ax.set_xticklabels(missing_by_country.columns, fontsize=8, rotation=90)  # Rotate labels for readability\n",
    "\n",
    "# Formatting\n",
    "plt.title(\"Missing Data Percentage by Country\", fontsize=14)\n",
    "plt.xlabel(\"Country\", fontsize=12)\n",
    "plt.ylabel(\"Variable\", fontsize=12)\n",
    "\n",
    "# Adjust layout to prevent label cutoff\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find variables where more than 30% of values are missing globally\n",
    "missing_threshold = 30  # Percentage threshold\n",
    "variables_to_exclude = missing_by_country[missing_by_country.mean(axis=1) > missing_threshold].index.tolist()\n",
    "\n",
    "# Print the variables with descriptions\n",
    "print(\"üö® Variables recommended for exclusion due to high missing rates:\")\n",
    "for var in variables_to_exclude:\n",
    "    description = field_description_lookup.get(var, \"No description available\")  # Lookup description or fallback\n",
    "    print(f\"- {var} {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find variables where more than 30% of values are missing globally\n",
    "missing_threshold = 30  # Percentage threshold\n",
    "variables_to_exclude = missing_by_country[missing_by_country.mean(axis=1) > missing_threshold].index.tolist()\n",
    "\n",
    "# Print the variables with descriptions\n",
    "print(\"üö® Variables recommended for exclusion due to high missing rates:\")\n",
    "for var in variables_to_exclude:\n",
    "    description = field_description_lookup.get(var, \"No description available\")  # Lookup description or fallback\n",
    "    print(f\"- {var}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of missing data per country\n",
    "missing_percentage = missing_by_country.mean(axis=0)  # Mean missing percentage per country\n",
    "data_completeness = 100 - missing_percentage  # Convert to completeness\n",
    "\n",
    "# Sort countries by completeness\n",
    "data_completeness_sorted = data_completeness.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the data completeness per country\n",
    "plt.figure(figsize=(12, 6))\n",
    "data_completeness_sorted.plot(kind=\"bar\", color=\"skyblue\", edgecolor=\"black\")\n",
    "\n",
    "# Formatting\n",
    "plt.xlabel(\"Country\")\n",
    "plt.ylabel(\"Data Completeness (%)\")\n",
    "plt.title(\"Data Completeness by Country (Higher is Better)\")\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels for readability\n",
    "plt.ylim(0, 100)  # Ensure scale is from 0% to 100%\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of missing values per country\n",
    "missing_percentage_by_country = missing_by_country.mean(axis=0)  # Mean missing % per country\n",
    "\n",
    "# Filter countries where the missing percentage is <= 30%\n",
    "countries_with_most_complete_data = missing_percentage_by_country[missing_percentage_by_country <= 35].index.tolist()\n",
    "\n",
    "# Print the countries with the most complete data\n",
    "print(\"üåç Countries with the most complete data (‚â§ 30% missing across all variables):\")\n",
    "for country in countries_with_most_complete_data:\n",
    "    print(f\"- {country}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of missing data per country\n",
    "missing_percentage = missing_by_country.mean(axis=0)  # Mean missing percentage per country\n",
    "data_completeness = 100 - missing_percentage  # Convert to completeness\n",
    "\n",
    "# Sort countries by completeness\n",
    "data_completeness_sorted = data_completeness.sort_values(ascending=False)\n",
    "data_completeness_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get memory usage of all DataFrames in the current notebook session\n",
    "def get_dataframe_memory_usage():\n",
    "    df_memory_usage = {}\n",
    "    \n",
    "    for var_name in globals():\n",
    "        var_value = globals()[var_name]\n",
    "        \n",
    "        if isinstance(var_value, pd.DataFrame):\n",
    "            memory_usage = var_value.memory_usage(deep=True).sum() / (1024 ** 2)  # Convert bytes to MB\n",
    "            df_memory_usage[var_name] = memory_usage\n",
    "    \n",
    "    # Convert to DataFrame for better readability\n",
    "    df_memory_usage_df = pd.DataFrame(list(df_memory_usage.items()), columns=[\"DataFrame\", \"Memory (MB)\"])\n",
    "    df_memory_usage_df = df_memory_usage_df.sort_values(by=\"Memory (MB)\", ascending=False)\n",
    "    \n",
    "    return df_memory_usage_df\n",
    "\n",
    "# Get the memory usage of all DataFrames\n",
    "df_memory_usage_report = get_dataframe_memory_usage()\n",
    "\n",
    "df_memory_usage_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "giyey2CkqSec",
    "outputId": "668eab2d-9397-47f2-a012-d0f9be15cf42"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Check for missing values\n",
    "#print(\"\\nMissing Values:\")\n",
    "#print(df.isnull().sum())\n",
    "\n",
    "# Memory friendly numeric column selection (Kernel always crashed before)\n",
    "numeric_cols = []\n",
    "for col in df.columns:\n",
    "    if pd.api.types.is_numeric_dtype(df[col]):\n",
    "        numeric_cols.append(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_numeric_columns_for_strings_with_samples(df, sample_size=5):\n",
    "    \"\"\"\n",
    "    Checks numeric columns of type 'object' for string values and prints samples if found.\n",
    "    \"\"\"\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        if df[col].apply(lambda x: isinstance(x, str)).any():\n",
    "            print(f\"üö® Mixed types found in column: {col}\")\n",
    "            mixed_samples = df.loc[df[col].apply(lambda x: isinstance(x, str)), col].sample(min(sample_size, len(df)), random_state=42)\n",
    "            print(\"Sample of non-numeric values:\")\n",
    "            print(mixed_samples.to_list())\n",
    "        else:\n",
    "            print(f\"‚úÖ No mixed types found in column: {col}\")\n",
    "\n",
    "# Example usage:\n",
    "check_numeric_columns_for_strings_with_samples(df, sample_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_variance = df.nunique()[df.nunique() == 1]\n",
    "print(\"Zero Variance Columns:\", zero_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_mixed_types_random_columns(df, num_columns=5):\n",
    "    \"\"\"\n",
    "    Checks for mixed data types in a random sample of columns in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to check.\n",
    "    num_columns (int): Number of random columns to check.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    sampled_columns = np.random.choice(df.columns, size=min(num_columns, len(df.columns)), replace=False)\n",
    "    \n",
    "    for col in sampled_columns:\n",
    "        mixed_types = df[col].apply(lambda x: not isinstance(x, (int, float, np.number)))\n",
    "        if mixed_types.any():\n",
    "            print(f\"Column '{col}' has mixed types:\")\n",
    "            print(df.loc[mixed_types, col].unique())\n",
    "        else:\n",
    "            print(f\"Column '{col}' has no mixed types.\")\n",
    "\n",
    "# Example usage:\n",
    "check_mixed_types_random_columns(df, num_columns=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "    # Exclude non-explanatory variables\n",
    "    exclude_columns = ['CNT', 'CNTSCHID', 'CNTSTUID', 'MATH_Proficient']\n",
    "    explanatory_vars = [col for col in numeric_cols if col not in exclude_columns]\n",
    "    \n",
    "    country_corr = df.groupby('CNT').apply(lambda x: x[explanatory_vars].corrwith(x['MATH_Proficient']))\n",
    "    \n",
    "    # Add a column for the average correlation across countries\n",
    "    country_corr['Average'] = country_corr.mean(axis=1)\n",
    "    \n",
    "    country_corr.to_csv('v3.0-country_correlation_results.csv', index=True)\n",
    "    print(\"‚úÖ Correlation results saved to 'v3.0-country_correlation_results.csv'\")\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    sns.heatmap(country_corr, cmap='coolwarm', annot=False, linewidths=0.5)\n",
    "    \n",
    "    plt.title(\"Correlation of Explanatory Variables with MATH_Proficient by Country\", fontsize=14)\n",
    "    plt.xlabel(\"Countries and Average\", fontsize=12)\n",
    "    plt.ylabel(\"Explanatory Variables\", fontsize=12)\n",
    "    plt.xticks(rotation=90, fontsize=8)\n",
    "    plt.yticks(fontsize=8)\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving for Simpsons Paradox!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "local_matrix = 'v3.0-average_correlation_df.csv'\n",
    "# Check if the file exists locally\n",
    "if os.path.exists(local_matrix):\n",
    "    print(\"üìÇ Loading data from local file...\")\n",
    "    average_correlation_df = pd.read_csv(local_matrix, index_col=0, usecols=None)\n",
    "else:\n",
    "    # List to store per-country correlation matrices\n",
    "    country_correlation_matrices = []\n",
    "    \n",
    "    # Compute correlation matrices for each country\n",
    "    for country, group in df.groupby(\"CNT\"):\n",
    "        country_corr_matrix = group[numeric_cols].corr()  # Full correlation matrix\n",
    "        country_correlation_matrices.append(country_corr_matrix)\n",
    "    \n",
    "    # Convert list to 3D numpy array (stacking matrices)\n",
    "    correlation_tensor = np.stack([df.values for df in country_correlation_matrices], axis=2)\n",
    "    \n",
    "    # Compute the mean correlation across countries (element-wise)\n",
    "    average_correlation_matrix = np.nanmean(correlation_tensor, axis=2)  # Averaging along country axis\n",
    "    \n",
    "    # Convert back to a DataFrame\n",
    "    average_correlation_df = pd.DataFrame(average_correlation_matrix, index=numeric_cols, columns=numeric_cols)\n",
    "    # Write it out for next time to save us 7 minutes\n",
    "    average_correlation_df.to_csv(local_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_correlation_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load or Perform Regular Correlation (ONLY against MATH_Proficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Plot correlation heatmap only for numeric variables\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "local_matrix_orig = 'v3.0-correlation_matrix.csv'\n",
    "# Check if the file exists locally\n",
    "if os.path.exists(local_matrix_orig):\n",
    "    print(\"üìÇ Loading data from local file...\")\n",
    "    correlation_matrix = pd.read_csv(local_matrix_orig, index_col=0, usecols=None)\n",
    "else:\n",
    "   # Exclude non-explanatory variables\n",
    "    exclude_columns = ['CNT', 'CNTSCHID', 'CNTSTUID']\n",
    "    explanatory_vars = [col for col in df.select_dtypes(include=['number']).columns if col not in exclude_columns]\n",
    "    # Calculate the correlation matrix including MATH_Proficient\n",
    "    correlation_matrix = df[explanatory_vars + ['MATH_Proficient']].corr()\n",
    "    correlation_matrix.to_csv(local_matrix_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Correlation (Not Averaging By Country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for strings (should be none in correlation matrix csv file after its loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for non-numeric values in correlation_matrix\n",
    "non_numeric = correlation_matrix.applymap(lambda x: not pd.api.types.is_number(x))\n",
    "\n",
    "# Filter and display non-numeric values\n",
    "for col in non_numeric.columns:\n",
    "    if non_numeric[col].any():\n",
    "        print(f\"üö® Non-numeric values found in column: {col}\")\n",
    "        print(correlation_matrix.loc[non_numeric[col], col].to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 753
    },
    "id": "RKXhKzfTyQE8",
    "outputId": "c7d9cd57-a71f-45a9-bff1-f32009d1e2da"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "def plot_heatmap_with_threshold(threshold=0.0):\n",
    "    filtered_corr = correlation_matrix.copy()\n",
    "\n",
    "    # Apply threshold across entire DataFrame\n",
    "    filtered_corr = filtered_corr.where(filtered_corr.abs() >= threshold, np.nan)  # Replace values below threshold with NaN\n",
    "\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.heatmap(filtered_corr, cmap='Blues', annot=False, linewidths=0.5)\n",
    "    plt.title(f\"Feature Correlation Heatmap (Threshold > {threshold})\", fontsize=14)\n",
    "    plt.xticks(fontsize=8, rotation=90)\n",
    "    plt.yticks(fontsize=8, rotation=0)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Interactive slider for threshold selection\n",
    "interact(plot_heatmap_with_threshold, threshold=widgets.FloatSlider(min=0.0, max=1.0, step=0.01, value=0.1, description=\"Threshold\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Averaging By Country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for strings (should be none in correlation matrix csv file after its loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for non-numeric values in correlation_matrix\n",
    "non_numeric = average_correlation_df.applymap(lambda x: not pd.api.types.is_number(x))\n",
    "\n",
    "# Filter and display non-numeric values\n",
    "for col in non_numeric.columns:\n",
    "    if non_numeric[col].any():\n",
    "        print(f\"üö® Non-numeric values found in column: {col}\")\n",
    "        print(average_correlation_df.loc[non_numeric[col], col].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "def plot_heatmap_with_threshold(threshold=0.0):\n",
    "    filtered_corr = average_correlation_df.copy()\n",
    "\n",
    "    # Apply threshold across entire DataFrame\n",
    "    filtered_corr = filtered_corr.where(filtered_corr.abs() >= threshold, np.nan)  # Replace values below threshold with NaN\n",
    "\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.heatmap(filtered_corr, cmap='Blues', annot=False, linewidths=0.5)\n",
    "    plt.title(f\"Feature Correlation Heatmap (Threshold > {threshold})\", fontsize=14)\n",
    "    plt.xticks(fontsize=8, rotation=90)\n",
    "    plt.yticks(fontsize=8, rotation=0)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Interactive slider for threshold selection\n",
    "interact(plot_heatmap_with_threshold, threshold=widgets.FloatSlider(min=0.0, max=1.0, step=0.01, value=0.1, description=\"Threshold\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without Averaging - Top Negative Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 989
    },
    "id": "SSUSgHLUbgoU",
    "outputId": "f35702e1-1279-423f-8669-5e1988b25498",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import textwrap\n",
    "\n",
    "# Analyze correlation with MATH_Proficient\n",
    "correlations = correlation_matrix[\"MATH_Proficient\"].sort_values()\n",
    "\n",
    "# Replace field codes with descriptions for display\n",
    "correlations_named = correlations.rename(index=lambda x: field_description_lookup.get(x, x))\n",
    "\n",
    "# Print Top Negative Correlations with Descriptions\n",
    "print(\"\\nTop Negative Correlations with MATH_Proficient:\")\n",
    "print(correlations_named.head(10))\n",
    "\n",
    "# Get top 10 negatively correlated features\n",
    "top_negative = correlations.head(10).index  # Keep original field IDs for selecting data\n",
    "\n",
    "# Create a mapping for renaming columns in the plot\n",
    "renamed_columns = {col: field_description_lookup.get(col, col) for col in top_negative}\n",
    "\n",
    "# Create histograms with wrapped text\n",
    "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(18, 8))  # Adjust grid size for spacing\n",
    "axes = axes.flatten()  # Flatten 2D array of axes to iterate easily\n",
    "\n",
    "for i, col in enumerate(top_negative):\n",
    "    ax = axes[i]\n",
    "    df[col].hist(ax=ax, bins=20)\n",
    "\n",
    "    # Wrap the title text\n",
    "    wrapped_title = \"\\n\".join(textwrap.wrap(renamed_columns.get(col, col), width=30))\n",
    "    ax.set_title(wrapped_title, fontsize=10)\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Distribution of Features Negatively Correlated with MATH_Proficient\", fontsize=14, y=1.05)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WITH Averaging - Top Negative Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import textwrap\n",
    "\n",
    "# Analyze correlation with MATH_Proficient\n",
    "correlations = average_correlation_df[\"MATH_Proficient\"].sort_values()\n",
    "\n",
    "# Replace field codes with descriptions for display\n",
    "correlations_named = correlations.rename(index=lambda x: field_description_lookup.get(x, x))\n",
    "\n",
    "# Print Top Negative Correlations with Descriptions\n",
    "print(\"\\nTop Negative Correlations with MATH_Proficient:\")\n",
    "print(correlations_named.head(10))\n",
    "\n",
    "# Get top 10 negatively correlated features\n",
    "top_negative = correlations.head(10).index  # Keep original field IDs for selecting data\n",
    "\n",
    "# Create a mapping for renaming columns in the plot\n",
    "renamed_columns = {col: field_description_lookup.get(col, col) for col in top_negative}\n",
    "\n",
    "# Create histograms with wrapped text\n",
    "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(18, 8))  # Adjust grid size for spacing\n",
    "axes = axes.flatten()  # Flatten 2D array of axes to iterate easily\n",
    "\n",
    "for i, col in enumerate(top_negative):\n",
    "    ax = axes[i]\n",
    "    df[col].hist(ax=ax, bins=20)\n",
    "\n",
    "    # Wrap the title text\n",
    "    wrapped_title = \"\\n\".join(textwrap.wrap(renamed_columns.get(col, col), width=30))\n",
    "    ax.set_title(wrapped_title, fontsize=10)\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Distribution of Features Negatively Correlated with MATH_Proficient\", fontsize=14, y=1.05)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without Averaging - Top Positive Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 992
    },
    "id": "tXXFXhua3KhE",
    "outputId": "bf527425-e249-42c5-b2a7-301579dd895a"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import textwrap\n",
    "\n",
    "# Analyze correlation with MATH_Proficient\n",
    "correlations = correlation_matrix[\"MATH_Proficient\"].sort_values()\n",
    "\n",
    "# Replace field codes with descriptions for display\n",
    "correlations_named = correlations.rename(index=lambda x: field_description_lookup.get(x, x))\n",
    "\n",
    "# Print Top Negative Correlations with Descriptions\n",
    "print(\"\\nTop Positive Correlations with MATH_Proficient:\")\n",
    "print(correlations_named.tail(10))\n",
    "\n",
    "# Get top 10 positively correlated features\n",
    "top_positive = correlations.tail(10).index  # Keep original field IDs for selecting data\n",
    "\n",
    "# Create a mapping for renaming columns in the plot\n",
    "renamed_columns = {col: field_description_lookup.get(col, col) for col in top_positive}\n",
    "\n",
    "# Create histograms with wrapped text\n",
    "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(18, 8))  # Adjust grid size for spacing\n",
    "axes = axes.flatten()  # Flatten 2D array of axes to iterate easily\n",
    "\n",
    "for i, col in enumerate(top_positive):\n",
    "    ax = axes[i]\n",
    "    df[col].hist(ax=ax, bins=20)\n",
    "\n",
    "    # Wrap the title text\n",
    "    wrapped_title = \"\\n\".join(textwrap.wrap(renamed_columns.get(col, col), width=30))\n",
    "    ax.set_title(wrapped_title, fontsize=10)\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Distribution of Features Positively Correlated with MATH_Proficient\", fontsize=14, y=1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WITH Averaging - Top Positive Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import textwrap\n",
    "\n",
    "# Analyze correlation with MATH_Proficient\n",
    "correlations = average_correlation_df[\"MATH_Proficient\"].sort_values()\n",
    "\n",
    "# Replace field codes with descriptions for display\n",
    "correlations_named = correlations.rename(index=lambda x: field_description_lookup.get(x, x))\n",
    "\n",
    "# Print Top Negative Correlations with Descriptions\n",
    "print(\"\\nTop Positive Correlations with MATH_Proficient:\")\n",
    "print(correlations_named.tail(10))\n",
    "\n",
    "# Get top 10 positively correlated features\n",
    "top_positive = correlations.tail(10).index  # Keep original field IDs for selecting data\n",
    "\n",
    "# Create a mapping for renaming columns in the plot\n",
    "renamed_columns = {col: field_description_lookup.get(col, col) for col in top_positive}\n",
    "\n",
    "# Create histograms with wrapped text\n",
    "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(18, 8))  # Adjust grid size for spacing\n",
    "axes = axes.flatten()  # Flatten 2D array of axes to iterate easily\n",
    "\n",
    "for i, col in enumerate(top_positive):\n",
    "    ax = axes[i]\n",
    "    df[col].hist(ax=ax, bins=20)\n",
    "\n",
    "    # Wrap the title text\n",
    "    wrapped_title = \"\\n\".join(textwrap.wrap(renamed_columns.get(col, col), width=30))\n",
    "    ax.set_title(wrapped_title, fontsize=10)\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Distribution of Features Positively Correlated with MATH_Proficient\", fontsize=14, y=1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "J0Fss1Gmu9YR",
    "outputId": "0074515f-f780-4303-99f8-bba54be63b9c"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for col in top_negative:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(df[df[\"MATH_Proficient\"] == 0][col], color=\"red\", label=\"Not Proficient\", kde=True, bins=20)\n",
    "    sns.histplot(df[df[\"MATH_Proficient\"] == 1][col], color=\"blue\", label=\"Proficient\", kde=True, bins=20)\n",
    "\n",
    "    wrapped_title = \"\\n\".join(textwrap.wrap(field_description_lookup.get(col, col), width=50))\n",
    "    plt.title(wrapped_title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "CRhq1t7f37wW",
    "outputId": "81392488-1596-47d6-d8ba-4492246c99e7"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for col in top_positive:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(df[df[\"MATH_Proficient\"] == 0][col], color=\"red\", label=\"Not Proficient\", kde=True, bins=20)\n",
    "    sns.histplot(df[df[\"MATH_Proficient\"] == 1][col], color=\"blue\", label=\"Proficient\", kde=True, bins=20)\n",
    "\n",
    "    wrapped_title = \"\\n\".join(textwrap.wrap(field_description_lookup.get(col, col), width=50))\n",
    "    plt.title(wrapped_title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
