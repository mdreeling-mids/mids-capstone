{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PISA 2022 Amazon SageMaker KNN\n",
    "\n",
    "More info on SageMaker Immersion Day: [Workshop Link](https://catalog.us-east-1.prod.workshops.aws/workshops/63069e26-921c-4ce1-9cc7-dd882ff62575/en-US/lab2-model-training/pro-code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Change country name below!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_name = 'United_States'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_name_edited = country_name.replace(\"_\", \"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "isConfigCell": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# cell 02\n",
    "import sagemaker\n",
    "bucket=sagemaker.Session().default_bucket()\n",
    "prefix = 'sagemaker/knn-'+country_name_edited\n",
    " \n",
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's bring in the Python libraries that we'll use throughout the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cell 03\n",
    "import numpy as np                                # For matrix operations and numerical processing\n",
    "import pandas as pd                               # For munging tabular data\n",
    "import matplotlib.pyplot as plt                   # For charts and visualizations\n",
    "from IPython.display import Image                 # For displaying images in the notebook\n",
    "from IPython.display import display               # For displaying outputs in the notebook\n",
    "from time import gmtime, strftime                 # For labeling SageMaker models, endpoints, etc.\n",
    "import sys                                        # For writing outputs to notebook\n",
    "import math                                       # For ceiling function\n",
    "import json                                       # For parsing hosting outputs\n",
    "import os                                         # For manipulating filepath names\n",
    "import sagemaker \n",
    "import zipfile     # Amazon SageMaker's Python SDK provides many helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download PISA 2022 Prepared Dataset\n",
    "\n",
    "This is our dataset output from our cleaned notebook [here](https://7z4vtvpqcoxouiu.studio.us-west-2.sagemaker.aws/jupyterlab/default/lab/tree/RTC%3Amids-capstone/notebooks/eda/Data_merging.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚òÅÔ∏è Downloading data from S3...\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# cell 06\n",
    "\n",
    "# Define local file path\n",
    "local_file_path = \"PISA_cleaned_dataset.csv\"  # Change as needed\n",
    "\n",
    "# Define S3 details\n",
    "bucket_name = \"sagemaker-us-west-2-986030204467\"\n",
    "file_key = \"capstone/testfiles/PISA_cleaned_dataset.csv\"\n",
    "\n",
    "# Check if the file exists locally\n",
    "if os.path.exists(local_file_path):\n",
    "    print(\"üìÇ Loading data from local file...\")\n",
    "    data = pd.read_csv(local_file_path, usecols=None)\n",
    "    \n",
    "else:\n",
    "    print(\"‚òÅÔ∏è Downloading data from S3...\")\n",
    "    \n",
    "    # Create S3 client\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "\n",
    "    # Download the file from S3\n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "\n",
    "    # Read the file into pandas DataFrame\n",
    "    data = pd.read_csv(response[\"Body\"], usecols=None)\n",
    "\n",
    "    # Save a local copy for future use\n",
    "    data.to_csv(local_file_path, index=False)\n",
    "    print(f\"‚úÖ File saved locally as {local_file_path}\")\n",
    "\n",
    "# Display first few rows\n",
    "#data.head()\n",
    "\n",
    "pd.set_option('display.max_columns', 500)     # Make sure we can see all of the columns\n",
    "pd.set_option('display.max_rows', 20)         # Keep the output on one page\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download dictionary for the variable names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the file from S3\n",
    "s3_client = boto3.client(\"s3\")\n",
    "dictionary_file = s3_client.get_object(Bucket=bucket_name, Key=\"capstone/testfiles/Variable_dictionary.csv\")\n",
    "\n",
    "# Read the file into pandas DataFrame\n",
    "dictionary = pd.read_csv(dictionary_file[\"Body\"], usecols=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subset the data to a specific COUNTRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = data[data['CNT'] == country_name]\n",
    "print(model_data.shape)\n",
    "model_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take out additional variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of columns to drop\n",
    "columns_to_remove = [\"CNT\", \"CNTSCHID\", \"CNTSTUID\", \"OECD\",\n",
    "    \"HOMEPOS\", \"RELATST\", \"BELONG\", \"BULLIED\", \"FEELSAFE\", \"SCHRISK\", \"PERSEVAGR\", \"CURIOAGR\", \n",
    "    \"COOPAGR\", \"EMPATAGR\", \"ASSERAGR\", \"STRESAGR\", \"EMOCOAGR\", \"GROSAGR\", \"INFOSEEK\", \"FAMSUP\", \n",
    "    \"DISCLIM\", \"TEACHSUP\", \"COGACRCO\", \"COGACMCO\", \"EXPOFA\", \"EXPO21ST\", \"MATHEFF\", \"MATHEF21\", \n",
    "    \"FAMCON\", \"ANXMAT\", \"MATHPERS\", \"CREATEFF\", \"CREATSCH\", \"CREATFAM\", \"CREATAS\", \"CREATOOS\", \n",
    "    \"CREATOP\", \"OPENART\", \"IMAGINE\", \"SCHSUST\", \"LEARRES\", \"PROBSELF\", \"FAMSUPSL\", \"FEELLAH\", \n",
    "    \"SDLEFF\", \"ICTRES\", \"FLSCHOOL\", \"FLMULTSB\", \"FLFAMILY\", \"ACCESSFP\", \"FLCONFIN\", \"FLCONICT\", \n",
    "    \"ACCESSFA\", \"ATTCONFM\", \"FRINFLFM\", \"ICTSCH\", \"ICTHOME\", \"ICTQUAL\", \"ICTSUBJ\", \"ICTENQ\", \n",
    "    \"ICTFEED\", \"ICTOUT\", \"ICTWKDY\", \"ICTWKEND\", \"ICTREG\", \"ICTINFO\", \"ICTEFFIC\", \"BODYIMA\", \n",
    "    \"SOCONPA\", \"LIFESAT\", \"PSYCHSYM\", \"SOCCON\", \"EXPWB\", \"CURSUPP\", \"PQMIMP\", \"PQMCAR\", \n",
    "    \"PARINVOL\", \"PQSCHOOL\", \"PASCHPOL\", \"ATTIMMP\", \"CREATHME\", \"CREATACT\", \"CREATOPN\", \n",
    "    \"CREATOR\", \"SCHAUTO\", \"TCHPART\", \"EDULEAD\", \"INSTLEAD\", \"ENCOURPG\", \"DIGDVPOL\", \"TEAFDBK\", \n",
    "    \"MTTRAIN\", \"DMCVIEWS\", \"NEGSCLIM\", \"STAFFSHORT\", \"EDUSHORT\", \"STUBEHA\", \"TEACHBEHA\", \n",
    "    \"STDTEST\", \"TDTEST\", \"ALLACTIV\", \"BCREATSC\", \"CREENVSC\", \"ACTCRESC\", \"OPENCUL\", \n",
    "    \"PROBSCRI\", \"SCPREPBP\", \"SCPREPAP\", \"DIGPREP\", \n",
    "    \"ESCS\", \"BMMJ1\", \"BFMJ2\", \"EFFORT1\", \"EFFORT2\", \"Option_UH\", \"SC209Q04JA\", \"SC209Q05JA\", \"SC209Q06JA\"\n",
    "]\n",
    "\n",
    "# Drop the columns above\n",
    "model_data = model_data.drop(columns=columns_to_remove, errors='ignore')  # `errors='ignore'` prevents errors if a column isn't found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_data.shape)\n",
    "model_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker's XGBoost container expects data in the libSVM or CSV data format.  **Note that the first column must be the target variable and the CSV should not include headers.**  Although repetitive, it's easiest to do this after the train|validation|test split rather than before.  This avoids any misalignment issues due to random reordering.\n",
    "* `MATH_Proficient`: Is the student falling behind in Math? (Average of 10 Math plausible values < 420.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get percent of students not proficient in Math\n",
    "proficient_n = (model_data['MATH_Proficient'] == 1).sum()\n",
    "not_proficient_n = (model_data['MATH_Proficient'] == 0).sum()\n",
    "not_proficient_p = round( not_proficient_n / (not_proficient_n + proficient_n) * 100, 1)\n",
    "print(\"Students who are NOT proficient in Math: \", not_proficient_n, \"(\", not_proficient_p, \"%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get imbalance ratio \n",
    "not_proficient_pp = not_proficient_n / (not_proficient_n + proficient_n)\n",
    "\n",
    "if not_proficient_pp < 0.5:\n",
    "    imbalance_ratio = (1 - not_proficient_pp) / not_proficient_pp\n",
    "else:\n",
    "    imbalance_ratio = not_proficient_pp / (1 - not_proficient_pp)\n",
    "    \n",
    "print(\"Imbalance ratio:\", round(imbalance_ratio,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns to bring 'MATH_Proficient' first\n",
    "new_order = ['MATH_Proficient'] + [col for col in model_data.columns if col != 'MATH_Proficient']\n",
    "model_data = model_data[new_order]\n",
    "\n",
    "# Get number of features\n",
    "n_features_original = model_data.shape[1]-1\n",
    "\n",
    "# Check the shape after dropping\n",
    "print(model_data.shape)\n",
    "\n",
    "model_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop columns with more than 20% missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***I commented out the code below because KNN might be able to work with datasets with missing values (like xgboost). If it yells at you that it can't handle missing values (which is what happened for linear learner), uncomment and run the codes below.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_data.dropna(thresh=int(0.8 * len(model_data)), axis=1, inplace=True)\n",
    "#print(model_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_features_final = model_data.shape[1]-1\n",
    "#print(\"Number of features (before dropping features with more than 20% missing):\", n_features_original)\n",
    "#print(\"Number of features (after dropping features with more than 20% missing):\", n_features_final)\n",
    "#print(\"Number of features with more than 20% missing:\", n_features_original - n_features_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For columns with less than 20% missing values, fill missing values with the median value of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_data.fillna(model_data.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll randomly split the data into 3 uneven groups.  **The model will be trained on 70% of data, it will then be evaluated on 15% of data to give us an estimate of the accuracy we hope to have on \"new\" data, and 15% will be held back as a final testing dataset which will be used later on.**\n",
    "\n",
    "A seed is included in the code so the splits can be replicated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cell 12\n",
    "# Randomly sort the data then split out first 70%, second 15%, and last 15%\n",
    "train_data, validation_data, test_data = np.split(model_data.sample(frac=1, random_state=1729), [int(0.7 * len(model_data)), int(0.85 * len(model_data))])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rows in FULL dataset:\", model_data.shape[0])\n",
    "\n",
    "train_data_percent = round(train_data.shape[0]/model_data.shape[0] * 100, 0)\n",
    "print(\"Number of rows in TRAINING dataset:\", train_data.shape[0], \"(\", train_data_percent, \"% )\")\n",
    "\n",
    "validation_data_percent = round(validation_data.shape[0]/model_data.shape[0] * 100, 0)\n",
    "print(\"Number of rows in VALIDATION dataset:\", validation_data.shape[0], \"(\", validation_data_percent, \"% )\")\n",
    "\n",
    "test_data_percent = round(test_data.shape[0]/model_data.shape[0] * 100, 0)\n",
    "print(\"Number of rows in TEST dataset:\", test_data.shape[0], \"(\", test_data_percent, \"% )\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save train dataset \n",
    "train_data.to_csv('train.csv', index=False, header=False)\n",
    "\n",
    "# Save validation dataset \n",
    "validation_data.to_csv('validation.csv', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data - Saved later to S3 as CSV\n",
    "print(train_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation data - Saved later to S3 as CSV\n",
    "print(validation_data.shape)\n",
    "validation_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data - NOT SAVED TO S3\n",
    "print(test_data.shape)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll copy the file to S3 for Amazon SageMaker's managed training to pickup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cell 14\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***In the code below, you should change \"xgboost\" to something that works for knn***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Set up SageMaker role and session\n",
    "role = get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Retrieve the built-in SageMaker KNN image\n",
    "container = sagemaker.image_uris.retrieve(\n",
    "    framework='knn', \n",
    "    region=boto3.Session().region_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, because we're training with the CSV file format, we'll create `s3_input`s that our training function can use as a pointer to the files in S3, which also specify that the content type is CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***In the code below, you might have to change \"text/csv\" to something else, depending on how knn works***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s3_input_train = sagemaker.inputs.TrainingInput(\n",
    "    s3_data='s3://{}/{}/train'.format(bucket, prefix), \n",
    "    content_type='text/csv'\n",
    ")\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(\n",
    "    s3_data='s3://{}/{}/validation'.format(bucket, prefix), \n",
    "    content_type='text/csv'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***In the code below, you should change \"linear-learner\" to something that works for knn***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Set up SageMaker role and session\n",
    "role = get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Retrieve the built-in SageMaker KNN image\n",
    "container = sagemaker.image_uris.retrieve(\n",
    "    framework='knn', \n",
    "    region=boto3.Session().region_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "knn_estimator.set_hyperparameters(\n",
    "    k=10,  # Number of nearest neighbors\n",
    "    sample_size=5000,  # Size of the sample used for training\n",
    "    predictor_type='classifier',  # 'classifier' or 'regressor'\n",
    "    feature_dim=feature_dim,  # Number of features\n",
    "    index_metric='COSINE'  # Distance metric\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use auto-tuning to find best hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***In the code below, change the hyperparameters to something that is relevant to KNN***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "mini_batch_upper_limit = int(train_data.shape[0]*0.16)\n",
    "\n",
    "hyperparameter_ranges = {'mini_batch_size': IntegerParameter(30, mini_batch_upper_limit),\n",
    "                         'learning_rate': ContinuousParameter(0.001, 0.01),\n",
    "                         'wd': ContinuousParameter(0.0001, 0.01),\n",
    "                         'l1': ContinuousParameter(0.0001, 0.01)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***In the code below, you might have to change \"validation:roc_auc_score\" to something else that works for KNN***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(estimator=knn,\n",
    "                            objective_metric_name='validation:roc_auc_score',\n",
    "                            hyperparameter_ranges=hyperparameter_ranges,\n",
    "                            max_jobs=50,  \n",
    "                            max_parallel_jobs=5)\n",
    "\n",
    "# May need to adjust number of jobs depending on budget!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s3_input_train = sagemaker.inputs.TrainingInput(\n",
    "    s3_data='s3://{}/{}/train'.format(bucket, prefix), \n",
    "    content_type='text/csv'\n",
    ")\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(\n",
    "    s3_data='s3://{}/{}/validation'.format(bucket, prefix), \n",
    "    content_type='text/csv'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 26\n",
    "boto3.client('sagemaker').describe_hyper_parameter_tuning_job(\n",
    "HyperParameterTuningJobName=tuner.latest_tuning_job.job_name)['HyperParameterTuningJobStatus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 27\n",
    "# Return the best training job name\n",
    "best_training_job = tuner.best_training_job()\n",
    "print(\"Best training job:\", best_training_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "knn_estimator.set_hyperparameters(\n",
    "    k=10,  # Number of nearest neighbors\n",
    "    sample_size=5000,  # Size of the sample used for training\n",
    "    predictor_type='classifier',  # 'classifier' or 'regressor'\n",
    "    feature_dim=feature_dim,  # Number of features\n",
    "    index_metric='COSINE'  # Distance metric\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model (the best model identified by HyperparameterTuner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "knn_predictor = knn_estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "knn_predictor.serializer = CSVSerializer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use a simple function to:\n",
    "1. Loop over our test dataset\n",
    "1. Split it into mini-batches of rows \n",
    "1. Convert those mini-batches to CSV string payloads (notice, we drop the target variable from our dataset first)\n",
    "1. Retrieve mini-batch predictions by invoking the XGBoost endpoint\n",
    "1. Collect predictions and convert from the CSV output our model provides into a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the raw prediction output\n",
    "raw_predictions = knn_predictor.predict(test_data.drop(['MATH_Proficient'], axis=1).to_numpy())\n",
    "\n",
    "# Decode and parse JSON\n",
    "parsed_predictions = json.loads(raw_predictions.decode(\"utf-8\"))\n",
    "\n",
    "# Extract the scores\n",
    "predictions = np.array([pred[\"score\"] for pred in parsed_predictions[\"predictions\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the real values for the test set\n",
    "real_values = test_data['MATH_Proficient']\n",
    "real_values.to_csv('real_values.csv', index=False, header=False)\n",
    "\n",
    "# Save the predicted values for the test set\n",
    "predicted_values_full = predictions\n",
    "predicted_values_full = pd.DataFrame(predicted_values_full, columns=['Predicted Values'])\n",
    "predicted_values_full.to_csv('predicted_values_full.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "knn_predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain the trained model using Clarify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "session = sagemaker.Session()\n",
    "\n",
    "model_name = \"Clarify-{}-{}\".format(country_name_edited, datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\"))\n",
    "\n",
    "best_model = sagemaker.estimator.Estimator.attach(best_training_job)  # Attach the best training job\n",
    "\n",
    "model = best_model.create_model(name=model_name)  # Create a model from the best job\n",
    "\n",
    "container_def = model.prepare_container_def()\n",
    "\n",
    "session.create_model(model_name, role, container_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = test_data.drop([\"MATH_Proficient\"], axis=1)\n",
    "test_target = test_data[\"MATH_Proficient\"]\n",
    "test_features.to_csv(\"test_features.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***In the code below, you might have to change \"text/csv\" to something else that works for KNN***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import clarify\n",
    "\n",
    "clarify_processor = clarify.SageMakerClarifyProcessor(\n",
    "    role=role, instance_count=1, instance_type=\"ml.m5.2xlarge\", sagemaker_session=session\n",
    ")\n",
    "\n",
    "model_config = clarify.ModelConfig(\n",
    "    model_name=model_name,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    instance_count=1,\n",
    "    accept_type=\"text/csv\",\n",
    "    content_type=\"text/csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "# Download data from S3 to local instance\n",
    "local_path = S3Downloader.download('s3://{}/{}/train'.format(bucket, prefix), './tmp/train_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and sample\n",
    "full_data = pd.read_csv('./tmp/train_data/train.csv', header=None)\n",
    "n = min(3000, len(full_data))  \n",
    "sampled_data = full_data.sample(n=n)  # If full_data has less than n, use the full sample\n",
    "\n",
    "# Save sampled data back to S3\n",
    "sampled_path = 'sampled_train_data.csv'\n",
    "sampled_data.to_csv(sampled_path, index=False)\n",
    "\n",
    "from sagemaker.s3 import S3Uploader\n",
    "sampled_s3_uri = S3Uploader.upload(sampled_path, 's3://{}/{}/sampled_train'.format(bucket, prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sampled_data.shape)\n",
    "sampled_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***In the code below, you might have to change \"text/csv\" to something else that works for KNN***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_config = clarify.SHAPConfig(\n",
    "    baseline=[test_features.iloc[0].values.tolist()],\n",
    "    num_samples=3000,  \n",
    "    agg_method=\"mean_abs\",\n",
    "    save_local_shap_values=True\n",
    ")\n",
    "\n",
    "explainability_output_path = \"s3://{}/{}/clarify-explainability\".format(bucket, prefix)\n",
    "\n",
    "explainability_data_config = clarify.DataConfig(\n",
    "    #s3_data_input_path='s3://{}/{}/train'.format(bucket, prefix),\n",
    "    s3_data_input_path=sampled_s3_uri,\n",
    "    s3_output_path=explainability_output_path,\n",
    "    label='MATH_Proficient',\n",
    "    headers=train_data.columns.to_list(),\n",
    "    dataset_type=\"text/csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set logging level for 'sagemaker.clarify' to WARNING (hides INFO messages)\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"sagemaker.clarify\").setLevel(logging.WARNING)\n",
    "\n",
    "clarify_processor.run_explainability(\n",
    "    data_config=explainability_data_config,\n",
    "    model_config=model_config,\n",
    "    explainability_config=shap_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model again with the top 20 predictors\n",
    "#### Get the list of top 20 predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your actual bucket name and prefix used in explainability_output_path\n",
    "# bucket = \"your-bucket-name\"\n",
    "# prefix = \"your-prefix\"  # e.g., the folder structure used in your explainability_output_path\n",
    "\n",
    "# Construct the S3 key for the output file\n",
    "key = f\"{prefix}/clarify-explainability/analysis.json\"\n",
    "\n",
    "# Initialize boto3 client for S3 and download the JSON report\n",
    "s3 = boto3.client(\"s3\")\n",
    "response = s3.get_object(Bucket=bucket, Key=key)\n",
    "content = response[\"Body\"].read().decode(\"utf-8\")\n",
    "report = json.loads(content)\n",
    "\n",
    "# Navigate to the global SHAP values dictionary\n",
    "global_shap = report[\"explanations\"][\"kernel_shap\"][\"label0\"][\"global_shap_values\"]\n",
    "\n",
    "# Sort the items by the SHAP value in descending order and take the top 20\n",
    "top_20 = sorted(global_shap.items(), key=lambda item: item[1], reverse=True)[:20]\n",
    "\n",
    "# Extract just the feature names\n",
    "top_20_features = [feature for feature, value in top_20]\n",
    "\n",
    "# Print\n",
    "print(\"Top 20 features with the highest mean absolute SHAP values:\")\n",
    "for feature in top_20_features:\n",
    "    print(feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a subset of the training dataset (with only 20 predictors)\n",
    "variables_to_keep = [\"MATH_Proficient\"] + top_20_features\n",
    "train_data_small = train_data[variables_to_keep]\n",
    "print(train_data_small.shape)\n",
    "train_data_small.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train dataset \n",
    "train_data_small.to_csv('train_small.csv', index=False, header=False)\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train_small/train_small.csv')).upload_file('train_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a subset of the validation dataset (with only 20 predictors)\n",
    "validation_data_small = validation_data[variables_to_keep]\n",
    "print(validation_data_small.shape)\n",
    "validation_data_small.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save validation dataset \n",
    "validation_data_small.to_csv('validation_small.csv', index=False, header=False)\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation_small/validation_small.csv')).upload_file('validation_small.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model using the hyperparameters from the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***In the code below, you should change \"xgboost\" to something else that works for KNN***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Set up SageMaker role and session\n",
    "role = get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Retrieve the built-in SageMaker KNN image\n",
    "container = sagemaker.image_uris.retrieve(\n",
    "    framework='knn', \n",
    "    region=boto3.Session().region_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***In the code below, you might have to change \"text/csv\" to something else that works for KNN***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s3_input_train = sagemaker.inputs.TrainingInput(\n",
    "    s3_data='s3://{}/{}/train'.format(bucket, prefix), \n",
    "    content_type='text/csv'\n",
    ")\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(\n",
    "    s3_data='s3://{}/{}/validation'.format(bucket, prefix), \n",
    "    content_type='text/csv'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "knn_estimator.set_hyperparameters(\n",
    "    k=10,  # Number of nearest neighbors\n",
    "    sample_size=5000,  # Size of the sample used for training\n",
    "    predictor_type='classifier',  # 'classifier' or 'regressor'\n",
    "    feature_dim=feature_dim,  # Number of features\n",
    "    index_metric='COSINE'  # Distance metric\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_small = test_data[variables_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 18\n",
    "knn_small_predictor = knn_small.deploy(initial_instance_count=1,\n",
    "                           instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 19\n",
    "knn_small_predictor.serializer = sagemaker.serializers.CSVSerializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use a simple function to:\n",
    "1. Loop over our test dataset\n",
    "1. Split it into mini-batches of rows \n",
    "1. Convert those mini-batches to CSV string payloads (notice, we drop the target variable from our dataset first)\n",
    "1. Retrieve mini-batch predictions by invoking the XGBoost endpoint\n",
    "1. Collect predictions and convert from the CSV output our model provides into a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the raw prediction output\n",
    "raw_predictions_small = knn_small_predictor.predict(test_data_small.drop(['MATH_Proficient'], axis=1).to_numpy())\n",
    "\n",
    "# Decode and parse JSON\n",
    "parsed_predictions_small = json.loads(raw_predictions_small.decode(\"utf-8\"))\n",
    "\n",
    "# Extract the scores\n",
    "predictions_small = np.array([pred[\"score\"] for pred in parsed_predictions_small[\"predictions\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predicted values for the test set\n",
    "predicted_values_small = predictions_small\n",
    "predicted_values_small = pd.DataFrame(predicted_values_small, columns=['Predicted Values'])\n",
    "predicted_values_small.to_csv('predicted_values_small.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "knn_small_predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of students not proficient in Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Students who are proficient: \", proficient_n)\n",
    "print(\"Students who are NOT proficient in Math: \", not_proficient_n, \"(\", not_proficient_p, \"%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model performance (model with all the predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggested_threshold = (100 - not_proficient_p)/100\n",
    "print(\"Suggested threshold:\", round(suggested_threshold, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Adjust the threhold for the FINAL PREDICTIONS if necessary!!*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will predict as Math_proficient if the probability is above this threhold. (If the threshold is above 0.5, it will reduce the number of students predicted as \"Math proficient\" for both students that are actually proficient and not proficient in Math.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.68\n",
    "\n",
    "print(\"Threshold:\", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read in the real values\n",
    "real_values = pd.read_csv('real_values.csv', usecols=[0], header=None)\n",
    "real_values = real_values.values.ravel()\n",
    "\n",
    "# Read in the predicted values (using the full model)\n",
    "predicted_values_full = pd.read_csv('predicted_values_full.csv', usecols=[0], header=None)\n",
    "predicted_values_full = predicted_values_full.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = pd.crosstab(index=real_values, \n",
    "                 columns=np.round( (predicted_values_full >= threshold).astype(int) ), \n",
    "                 rownames=['actuals'], \n",
    "                 colnames=['predictions'])\n",
    "\n",
    "TN = cm.loc[0.0, 0.0]\n",
    "FP = cm.loc[0.0, 1.0]\n",
    "FN = cm.loc[1.0, 0.0]\n",
    "TP = cm.loc[1.0, 1.0]\n",
    "\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN) * 100\n",
    "precision = TP / (TP + FP) * 100 if (TP + FP) > 0 else 0\n",
    "recall = TP / (TP + FN) * 100 if (TP + FN) > 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "specificity = TN / (TN + FP) * 100 if (TN + FP) > 0 else 0\n",
    "\n",
    "print(\"MODEL USING ALL FEATURES \\n\")\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nAccuracy: {:.1f}\".format(accuracy))\n",
    "print(\"F1 Score: {:.1f}\".format(f1_score))\n",
    "print(\"Precision: {:.1f}\".format(precision))\n",
    "print(\"Recall: {:.1f}\".format(recall))\n",
    "print(\"Specificity: {:.1f}\".format(specificity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance (model with 20 predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the predicted values (using 20 predictors)\n",
    "predicted_values_small = pd.read_csv('predicted_values_small.csv', usecols=[0], header=None)\n",
    "predicted_values_small = predicted_values_small.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_small = pd.crosstab(index=real_values, \n",
    "                       columns=np.round( (predicted_values_small >= threshold).astype(int) ), \n",
    "                       rownames=['actuals'], \n",
    "                       colnames=['predictions'])\n",
    "\n",
    "TN_small = cm_small.loc[0.0, 0.0]\n",
    "FP_small = cm_small.loc[0.0, 1.0]\n",
    "FN_small = cm_small.loc[1.0, 0.0]\n",
    "TP_small = cm_small.loc[1.0, 1.0]\n",
    "\n",
    "accuracy_small = (TP_small + TN_small) / (TP_small + TN_small + FP_small + FN_small) * 100\n",
    "precision_small = TP_small / (TP_small + FP_small) * 100 if (TP_small + FP_small) > 0 else 0\n",
    "recall_small = TP_small / (TP_small + FN_small) * 100 if (TP_small + FN_small) > 0 else 0\n",
    "f1_score_small = 2 * (precision_small * recall_small) / (precision_small + recall_small) if (precision_small + recall_small) > 0 else 0\n",
    "specificity_small = TN_small / (TN_small + FP_small) * 100 if (TN_small + FP_small) > 0 else 0\n",
    "\n",
    "print(\"MODEL USING 20 FEATURES \\n\")\n",
    "print(cm_small)\n",
    "\n",
    "print(\"\\nAccuracy: {:.1f}\".format(accuracy_small))\n",
    "print(\"F1 Score: {:.1f}\".format(f1_score_small))\n",
    "print(\"Precision: {:.1f}\".format(precision_small))\n",
    "print(\"Recall: {:.1f}\".format(recall_small))\n",
    "print(\"Specificity: {:.1f}\".format(specificity_small))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 20 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Filter the DataFrame to only include rows where Variable_name is in top_20_features\n",
    "top_20_dictionary = dictionary[dictionary[\"Variable_name\"].isin(top_20_features)]\n",
    "top_20_table = top_20_dictionary.set_index(\"Variable_name\").loc[top_20_features].reset_index()\n",
    "display(Markdown(top_20_table.to_markdown()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
