{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy a Keras or Tensorflow model trained anywhere using Amazon SageMaker\n",
    "\n",
    "\n",
    "Amazon SageMaker makes it easier for any developer or data scientist to build, train, and deploy machine learning (ML) models. While it’s designed to alleviate the undifferentiated heavy lifting from the full life cycle of ML models, Amazon SageMaker’s capabilities can also be used independently of one another; that is, models trained in Amazon SageMaker can be optimized and deployed outside of Amazon SageMaker including edge (mobile or IoT devices). Conversely, Amazon SageMaker can deploy and host pre-trained models such as model zoos or models trained locally by your team. \n",
    "\n",
    "In this notebook, we’ll demonstrate how to deploy a trained Keras (Tensorflow backend) model using Amazon SageMaker, taking advantage of Amazon SageMaker deployment features, such as selecting the type and number of instances, model compilation to improve inference latency, and autoscaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Set up\n",
    "\n",
    "In the AWS Management Console, go to the Amazon SageMaker console. Choose Notebook Instances, and create a new notebook instance. Upload the current notebook and set the kernel to ``conda_tensorflow_p36``.\n",
    "\n",
    "The get_execution_role function retrieves the AWS Identity and Access Management (IAM) role you created at the time of creating your notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade \"sagemaker>=2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker import Session\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = Session()\n",
    "region = sess.boto_region_name\n",
    "bucket = sess.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'Global'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.228.0\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker==2.242.0\n",
      "  Using cached sagemaker-2.242.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.242.0) (23.2.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.35.75 in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.242.0) (1.36.3)\n",
      "Requirement already satisfied: cloudpickle>=2.2.1 in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.242.0) (2.2.1)\n",
      "Requirement already satisfied: docker in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.242.0) (7.1.0)\n",
      "Requirement already satisfied: fastapi in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.242.0) (0.115.8)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.242.0) (0.2.0)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.242.0) (6.10.0)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.242.0) (4.23.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.242.0) (1.26.4)\n",
      "Requirement already satisfied: omegaconf<=2.3,>=2.2 in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.242.0) (2.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.242.0) (24.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.242.0) (2.2.3)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.242.0) (0.3.3)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.242.0) (4.3.6)\n",
      "Requirement already satisfied: protobuf<6.0,>=3.12 in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.242.0) (4.25.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.242.0) (5.9.8)\n",
      "Requirement already satisfied: pyyaml~=6.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.242.0) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.242.0) (2.32.3)\n",
      "Collecting sagemaker-core<2.0.0,>=1.0.17 (from sagemaker==2.242.0)\n",
      "  Using cached sagemaker_core-1.0.27-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.242.0) (0.7.7)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.242.0) (1.0.1)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.242.0) (2.0.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.242.0) (4.67.1)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.242.0) (1.26.19)\n",
      "Requirement already satisfied: uvicorn in /opt/conda/lib/python3.11/site-packages (from sagemaker==2.242.0) (0.34.0)\n",
      "Requirement already satisfied: botocore<1.37.0,>=1.36.3 in /opt/conda/lib/python3.11/site-packages (from boto3<2.0,>=1.35.75->sagemaker==2.242.0) (1.36.3)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from boto3<2.0,>=1.35.75->sagemaker==2.242.0) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /opt/conda/lib/python3.11/site-packages (from boto3<2.0,>=1.35.75->sagemaker==2.242.0) (0.11.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker==2.242.0) (3.21.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /opt/conda/lib/python3.11/site-packages (from omegaconf<=2.3,>=2.2->sagemaker==2.242.0) (4.9.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker==2.242.0) (2.10.6)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker==2.242.0) (13.9.4)\n",
      "Collecting mock<5.0,>4.0 (from sagemaker-core<2.0.0,>=1.0.17->sagemaker==2.242.0)\n",
      "  Using cached mock-4.0.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.11/site-packages (from jsonschema->sagemaker==2.242.0) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.11/site-packages (from jsonschema->sagemaker==2.242.0) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from jsonschema->sagemaker==2.242.0) (0.22.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->sagemaker==2.242.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->sagemaker==2.242.0) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->sagemaker==2.242.0) (2024.12.14)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /opt/conda/lib/python3.11/site-packages (from fastapi->sagemaker==2.242.0) (0.45.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from fastapi->sagemaker==2.242.0) (4.12.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from google-pasta->sagemaker==2.242.0) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->sagemaker==2.242.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->sagemaker==2.242.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->sagemaker==2.242.0) (2025.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.9 in /opt/conda/lib/python3.11/site-packages (from pathos->sagemaker==2.242.0) (1.7.6.9)\n",
      "Requirement already satisfied: dill>=0.3.9 in /opt/conda/lib/python3.11/site-packages (from pathos->sagemaker==2.242.0) (0.3.9)\n",
      "Requirement already satisfied: pox>=0.3.5 in /opt/conda/lib/python3.11/site-packages (from pathos->sagemaker==2.242.0) (0.3.5)\n",
      "Requirement already satisfied: multiprocess>=0.70.17 in /opt/conda/lib/python3.11/site-packages (from pathos->sagemaker==2.242.0) (0.70.17)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.11/site-packages (from uvicorn->sagemaker==2.242.0) (8.1.8)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.11/site-packages (from uvicorn->sagemaker==2.242.0) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker==2.242.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/conda/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker==2.242.0) (2.27.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker==2.242.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker==2.242.0) (2.19.1)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /opt/conda/lib/python3.11/site-packages (from starlette<0.46.0,>=0.40.0->fastapi->sagemaker==2.242.0) (4.8.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.11/site-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi->sagemaker==2.242.0) (1.3.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker==2.242.0) (0.1.2)\n",
      "Using cached sagemaker-2.242.0-py3-none-any.whl (1.6 MB)\n",
      "Using cached sagemaker_core-1.0.27-py3-none-any.whl (407 kB)\n",
      "Using cached mock-4.0.3-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: mock, sagemaker-core, sagemaker\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.228.0\n",
      "    Uninstalling sagemaker-2.228.0:\n",
      "      Successfully uninstalled sagemaker-2.228.0\n",
      "Successfully installed mock-4.0.3 sagemaker-2.242.0 sagemaker-core-1.0.27\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker==2.242.0 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.18.0\n",
      "  Using cached tensorflow-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.18.0) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.18.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.18.0) (25.1.24)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.18.0) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.18.0) (0.2.0)\n",
      "Collecting libclang>=13.0.0 (from tensorflow==2.18.0)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.18.0) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.18.0) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.18.0) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.18.0) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.18.0) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.18.0) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.18.0) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.18.0) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.18.0) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.18.0) (1.62.2)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow==2.18.0)\n",
      "  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: keras>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.18.0) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.18.0) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.18.0) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow==2.18.0) (0.4.0)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow==2.18.0)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow==2.18.0) (13.9.4)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow==2.18.0) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow==2.18.0) (0.14.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (0.7.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow==2.18.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow==2.18.0) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.18.0) (0.1.2)\n",
      "Using cached tensorflow-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.4 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "Using cached tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "Installing collected packages: libclang, tensorflow-io-gcs-filesystem, tensorboard, tensorflow\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.17.1\n",
      "    Uninstalling tensorboard-2.17.1:\n",
      "      Successfully uninstalled tensorboard-2.17.1\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.17.0\n",
      "    Uninstalling tensorflow-2.17.0:\n",
      "      Successfully uninstalled tensorflow-2.17.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 1.2 requires nvidia-ml-py3==7.352.0, which is not installed.\n",
      "autogluon-multimodal 1.2 requires jsonschema<4.22,>=4.18, but you have jsonschema 4.23.0 which is incompatible.\n",
      "autogluon-multimodal 1.2 requires nltk<3.9,>=3.4.5, but you have nltk 3.9.1 which is incompatible.\n",
      "autogluon-multimodal 1.2 requires omegaconf<2.3.0,>=2.1.1, but you have omegaconf 2.3.0 which is incompatible.\n",
      "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed libclang-18.1.1 tensorboard-2.18.0 tensorflow-2.18.0 tensorflow-io-gcs-filesystem-0.37.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow==2.18.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running this locally, check your version of Tensorflow to prevent downstream framework errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 20:58:26.840205: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-01 20:58:26.843739: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-01 20:58:26.854826: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743541106.875003     183 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743541106.880857     183 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-01 20:58:26.900866: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)  # This notebook runs on TensorFlow 1.15.x or earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_framework_version = tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary Python packages and install the version of h5py for compatibility with your Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting h5py==2.10.0\n",
      "  Using cached h5py-2.10.0.tar.gz (301 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /opt/conda/lib/python3.11/site-packages (from h5py==2.10.0) (1.26.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from h5py==2.10.0) (1.17.0)\n",
      "Building wheels for collected packages: h5py\n",
      "  Building wheel for h5py (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[85 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m /opt/conda/lib/python3.11/site-packages/setuptools/__init__.py:94: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         Requirements should be satisfied by a PEP 517 installer.\n",
      "  \u001b[31m   \u001b[0m         If you are using pip, you can try `pip install --use-pep517`.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   dist.fetch_build_eggs(dist.setup_requires)\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311/h5py\n",
      "  \u001b[31m   \u001b[0m copying h5py/__init__.py -> build/lib.linux-x86_64-cpython-311/h5py\n",
      "  \u001b[31m   \u001b[0m copying h5py/h5py_warnings.py -> build/lib.linux-x86_64-cpython-311/h5py\n",
      "  \u001b[31m   \u001b[0m copying h5py/highlevel.py -> build/lib.linux-x86_64-cpython-311/h5py\n",
      "  \u001b[31m   \u001b[0m copying h5py/ipy_completer.py -> build/lib.linux-x86_64-cpython-311/h5py\n",
      "  \u001b[31m   \u001b[0m copying h5py/version.py -> build/lib.linux-x86_64-cpython-311/h5py\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311/h5py/_hl\n",
      "  \u001b[31m   \u001b[0m copying h5py/_hl/__init__.py -> build/lib.linux-x86_64-cpython-311/h5py/_hl\n",
      "  \u001b[31m   \u001b[0m copying h5py/_hl/attrs.py -> build/lib.linux-x86_64-cpython-311/h5py/_hl\n",
      "  \u001b[31m   \u001b[0m copying h5py/_hl/base.py -> build/lib.linux-x86_64-cpython-311/h5py/_hl\n",
      "  \u001b[31m   \u001b[0m copying h5py/_hl/compat.py -> build/lib.linux-x86_64-cpython-311/h5py/_hl\n",
      "  \u001b[31m   \u001b[0m copying h5py/_hl/dataset.py -> build/lib.linux-x86_64-cpython-311/h5py/_hl\n",
      "  \u001b[31m   \u001b[0m copying h5py/_hl/datatype.py -> build/lib.linux-x86_64-cpython-311/h5py/_hl\n",
      "  \u001b[31m   \u001b[0m copying h5py/_hl/dims.py -> build/lib.linux-x86_64-cpython-311/h5py/_hl\n",
      "  \u001b[31m   \u001b[0m copying h5py/_hl/files.py -> build/lib.linux-x86_64-cpython-311/h5py/_hl\n",
      "  \u001b[31m   \u001b[0m copying h5py/_hl/filters.py -> build/lib.linux-x86_64-cpython-311/h5py/_hl\n",
      "  \u001b[31m   \u001b[0m copying h5py/_hl/group.py -> build/lib.linux-x86_64-cpython-311/h5py/_hl\n",
      "  \u001b[31m   \u001b[0m copying h5py/_hl/selections.py -> build/lib.linux-x86_64-cpython-311/h5py/_hl\n",
      "  \u001b[31m   \u001b[0m copying h5py/_hl/selections2.py -> build/lib.linux-x86_64-cpython-311/h5py/_hl\n",
      "  \u001b[31m   \u001b[0m copying h5py/_hl/vds.py -> build/lib.linux-x86_64-cpython-311/h5py/_hl\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311/h5py/tests\n",
      "  \u001b[31m   \u001b[0m copying h5py/tests/__init__.py -> build/lib.linux-x86_64-cpython-311/h5py/tests\n",
      "  \u001b[31m   \u001b[0m copying h5py/tests/common.py -> build/lib.linux-x86_64-cpython-311/h5py/tests\n",
      "  \u001b[31m   \u001b[0m copying h5py/tests/test_attribute_create.py -> build/lib.linux-x86_64-cpython-311/h5py/tests\n",
      "  \u001b[31m   \u001b[0m copying h5py/tests/test_attrs.py -> build/lib.linux-x86_64-cpython-311/h5py/tests\n",
      "  \u001b[31m   \u001b[0m copying h5py/tests/test_attrs_data.py -> build/lib.linux-x86_64-cpython-311/h5py/tests\n",
      "  \u001b[31m   \u001b[0m copying h5py/tests/test_base.py -> build/lib.linux-x86_64-cpython-311/h5py/tests\n",
      "  \u001b[31m   \u001b[0m copying h5py/tests/test_completions.py -> build/lib.linux-x86_64-cpython-311/h5py/tests\n",
      "  \u001b[31m   \u001b[0m copying h5py/tests/test_dataset.py -> build/lib.linux-x86_64-cpython-311/h5py/tests\n",
      "  \u001b[31m   \u001b[0m copying h5py/tests/test_dataset_getitem.py -> build/lib.linux-x86_64-cpython-311/h5py/tests\n",
      "  \u001b[31m   \u001b[0m copying h5py/tests/test_dataset_swmr.py -> build/lib.linux-x86_64-cpython-311/h5py/tests\n",
      "  \u001b[31m   \u001b[0m copying h5py/tests/test_datatype.py -> build/lib.linux-x86_64-cpython-311/h5py/tests\n",
      "  \u001b[31m   \u001b[0m copying h5py/tests/test_deprecation.py -> build/lib.linux-x86_64-cpython-311/h5py/tests\n",
      "  \u001b[31m   \u001b[0m copying h5py/tests/test_dimension_scales.py -> build/lib.linux-x86_64-cpython-311/h5py/tests\n",
      "  \u001b[31m   \u001b[0m copying h5py/tests/test_dims_dimensionproxy.py -> build/lib.linux-x86_64-cpython-311/h5py/tests\n",
      "  \u001b[31m   \u001b[0m copying h5py/tests/test_dtype.py -> build/lib.linux-x86_64-cpython-311/h5py/tests\n",
      "  \u001b[31m   \u001b[0m copying h5py/tests/test_file.py -> build/lib.linux-x86_64-cpython-311/h5py/tests\n",
      "  \u001b[31m   \u001b[0m copying h5py/tests/test_file2.py -> build/lib.linux-x86_64-cpython-311/h5py/tests\n",
      "  \u001b[31m   \u001b[0m copying h5py/tests/test_file_image.py -> build/lib.linux-x86_64-cpython-311/h5py/tests\n",
      "  \u001b[31m   \u001b[0m copying h5py/tests/test_filters.py -> build/lib.linux-x86_64-cpython-311/h5py/tests\n",
      "  \u001b[31m   \u001b[0m copying h5py/tests/test_group.py -> build/lib.linux-x86_64-cpython-311/h5py/tests\n",
      "  \u001b[31m   \u001b[0m copying h5py/tests/test_h5.py -> build/lib.linux-x86_64-cpython-311/h5py/tests\n",
      "  \u001b[31m   \u001b[0m copying h5py/tests/test_h5d_direct_chunk.py -> build/lib.linux-x86_64-cpython-311/h5py/tests\n",
      "  \u001b[31m   \u001b[0m copying h5py/tests/test_h5f.py -> build/lib.linux-x86_64-cpython-311/h5py/tests\n",
      "  \u001b[31m   \u001b[0m copying h5py/tests/test_h5p.py -> build/lib.linux-x86_64-cpython-311/h5py/tests\n",
      "  \u001b[31m   \u001b[0m copying h5py/tests/test_h5pl.py -> build/lib.linux-x86_64-cpython-311/h5py/tests\n",
      "  \u001b[31m   \u001b[0m copying h5py/tests/test_h5t.py -> build/lib.linux-x86_64-cpython-311/h5py/tests\n",
      "  \u001b[31m   \u001b[0m copying h5py/tests/test_objects.py -> build/lib.linux-x86_64-cpython-311/h5py/tests\n",
      "  \u001b[31m   \u001b[0m copying h5py/tests/test_selections.py -> build/lib.linux-x86_64-cpython-311/h5py/tests\n",
      "  \u001b[31m   \u001b[0m copying h5py/tests/test_slicing.py -> build/lib.linux-x86_64-cpython-311/h5py/tests\n",
      "  \u001b[31m   \u001b[0m copying h5py/tests/test_threads.py -> build/lib.linux-x86_64-cpython-311/h5py/tests\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311/h5py/tests/test_vds\n",
      "  \u001b[31m   \u001b[0m copying h5py/tests/test_vds/__init__.py -> build/lib.linux-x86_64-cpython-311/h5py/tests/test_vds\n",
      "  \u001b[31m   \u001b[0m copying h5py/tests/test_vds/test_highlevel_vds.py -> build/lib.linux-x86_64-cpython-311/h5py/tests/test_vds\n",
      "  \u001b[31m   \u001b[0m copying h5py/tests/test_vds/test_lowlevel_vds.py -> build/lib.linux-x86_64-cpython-311/h5py/tests/test_vds\n",
      "  \u001b[31m   \u001b[0m copying h5py/tests/test_vds/test_virtual_source.py -> build/lib.linux-x86_64-cpython-311/h5py/tests/test_vds\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m Loading library to get version: libhdf5.so\n",
      "  \u001b[31m   \u001b[0m Autodetected HDF5 1.14.4\n",
      "  \u001b[31m   \u001b[0m ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m                        Summary of the h5py configuration\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m     Path to HDF5: None\n",
      "  \u001b[31m   \u001b[0m     HDF5 Version: '1.14.4'\n",
      "  \u001b[31m   \u001b[0m      MPI Enabled: False\n",
      "  \u001b[31m   \u001b[0m Rebuild Required: True\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m Executing api_gen rebuild of defs\n",
      "  \u001b[31m   \u001b[0m Executing cythonize()\n",
      "  \u001b[31m   \u001b[0m h5py requires pkg-config unless the HDF5 path is explicitly specified\n",
      "  \u001b[31m   \u001b[0m error: pkg-config probably not installed: FileNotFoundError(2, 'No such file or directory')\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for h5py\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for h5py\n",
      "Failed to build h5py\n",
      "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (h5py)\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# ref: https://github.com/keras-team/keras/issues/14265\n",
    "!pip install \"h5py==2.10.0\"\n",
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Load the Keras model using the json and weights file\n",
    "\n",
    "If you saved your model in the TensorFlow ProtoBuf format, skip to \"Step 4. Convert the TensorFlow model to an Amazon SageMaker-readable format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a directory called ``keras_model``, download [hosted keras model](https://s3.amazonaws.com/aws-ml-blog/artifacts/keras-tensorflow-model-deployment/model.zip), and unzip the model.json and model-weights.h5 files to ``keras_model``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir keras_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://s3.amazonaws.com/aws-ml-blog/artifacts/keras-tensorflow-model-deployment/model.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip model.zip -d keras_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#import tensorflow as tf\n",
    "#import tensorflow.keras as keras\n",
    "#from keras.models import model_from_json\n",
    "\n",
    "#with open(os.path.join('keras_model', 'model.json'), 'r') as fp:\n",
    "#    loaded_model_json = fp.read()\n",
    "#loaded_model = model_from_json(loaded_model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_model.load_weights('keras_model/model-weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Export the Keras model to the TensorFlow ProtoBuf format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.python.saved_model import builder\n",
    "#from tensorflow.python.saved_model.signature_def_utils import predict_signature_def\n",
    "#from tensorflow.python.saved_model import tag_constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This directory structure will need to be followed - see notes for the next section\n",
    "#model_version = '1'\n",
    "#export_dir = 'export/Servo/' + model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Protocol Buffer SavedModel at 'export_dir'\n",
    "#builder = builder.SavedModelBuilder(export_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prediction signature to be used by TensorFlow Serving Predict API\n",
    "#signature = predict_signature_def(\n",
    "#    inputs={\"inputs\": loaded_model.input}, outputs={\"score\": loaded_model.output})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#session = tf.compat.v1.Session()\n",
    "#init_op = tf.compat.v1.global_variables_initializer()\n",
    "#session.run(init_op)\n",
    "# Save the meta graph and variables\n",
    "#builder.add_meta_graph_and_variables(\n",
    "#    sess=session, tags=[tag_constants.SERVING], signature_def_map={\"serving_default\": signature})\n",
    "#builder.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3-1 Repack/Convert the Colab Model Tarballs so that they match MultiModel Upload Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import shutil\n",
    "\n",
    "SOURCE_DIR = \"models_for_repack\"\n",
    "DEST_DIR = \"models_for_deploy\"\n",
    "TMP_EXTRACT_DIR = \"tmp_extract\"\n",
    "TMP_REPACK_DIR = \"tmp_repack\"\n",
    "\n",
    "def repack_all_models_for_mme(source_dir=SOURCE_DIR, dest_dir=DEST_DIR):\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "    os.makedirs(TMP_EXTRACT_DIR, exist_ok=True)\n",
    "    os.makedirs(TMP_REPACK_DIR, exist_ok=True)\n",
    "\n",
    "    model_files = [f for f in os.listdir(source_dir) if f.startswith(\"saved_model_\") and f.endswith(\".tar.gz\")]\n",
    "\n",
    "    for model_file in model_files:\n",
    "        # Extract country name: saved_model_United_States.tar.gz → United_States\n",
    "        country = model_file.replace(\"saved_model_\", \"\").replace(\".tar.gz\", \"\")\n",
    "        input_tar_path = os.path.join(source_dir, model_file)\n",
    "        output_tar_path = os.path.join(dest_dir, f\"{country}.model.tar.gz\")\n",
    "\n",
    "        print(f\"📦 Repacking: {model_file} → {country}.model.tar.gz\")\n",
    "\n",
    "        # Clean temp dirs\n",
    "        shutil.rmtree(TMP_EXTRACT_DIR, ignore_errors=True)\n",
    "        shutil.rmtree(TMP_REPACK_DIR, ignore_errors=True)\n",
    "        os.makedirs(TMP_EXTRACT_DIR, exist_ok=True)\n",
    "        os.makedirs(TMP_REPACK_DIR, exist_ok=True)\n",
    "\n",
    "        # Step 1: Extract original tar\n",
    "        with tarfile.open(input_tar_path, \"r:gz\") as tar:\n",
    "            tar.extractall(TMP_EXTRACT_DIR)\n",
    "\n",
    "        # Step 2: Find the only subfolder\n",
    "        subfolders = [f for f in os.listdir(TMP_EXTRACT_DIR) if os.path.isdir(os.path.join(TMP_EXTRACT_DIR, f))]\n",
    "        if len(subfolders) != 1:\n",
    "            raise RuntimeError(f\"❌ Expected 1 subfolder inside {model_file}, found: {subfolders}\")\n",
    "\n",
    "        original_model_path = os.path.join(TMP_EXTRACT_DIR, subfolders[0])\n",
    "        repack_target_path = os.path.join(TMP_REPACK_DIR, \"1\")\n",
    "\n",
    "        # Step 3: Copy contents into a \"1/\" folder\n",
    "        shutil.copytree(original_model_path, repack_target_path)\n",
    "\n",
    "        # Step 4: Create new MME-ready tar.gz with root \"1/\"\n",
    "        with tarfile.open(output_tar_path, \"w:gz\") as tar:\n",
    "            tar.add(repack_target_path, arcname=\"1\")\n",
    "\n",
    "        print(f\"✅ Saved: {output_tar_path}\")\n",
    "\n",
    "    # Cleanup temp dirs\n",
    "    shutil.rmtree(TMP_EXTRACT_DIR, ignore_errors=True)\n",
    "    shutil.rmtree(TMP_REPACK_DIR, ignore_errors=True)\n",
    "\n",
    "    print(\"🎉 All models repacked and saved to:\", dest_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Repacking: saved_model_Global.tar.gz → Global.model.tar.gz\n",
      "✅ Saved: models_for_deploy/Global.model.tar.gz\n",
      "🎉 All models repacked and saved to: models_for_deploy\n"
     ]
    }
   ],
   "source": [
    "repack_all_models_for_mme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def extract_and_inspect_model(tar_path, extract_dir=\"tmp_inspect\"):\n",
    "    # Clean up old temp dir\n",
    "    shutil.rmtree(extract_dir, ignore_errors=True)\n",
    "    os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "    # Extract tar.gz\n",
    "    with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "        tar.extractall(extract_dir)\n",
    "\n",
    "    # Find model directory\n",
    "    for root, dirs, files in os.walk(extract_dir):\n",
    "        if \"saved_model.pb\" in files:\n",
    "            model_path = root\n",
    "            break\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No saved_model.pb found in extracted archive.\")\n",
    "\n",
    "    print(f\"✅ Found SavedModel at: {model_path}\")\n",
    "    \n",
    "    # Run saved_model_cli\n",
    "    os.system(f\"saved_model_cli show --all --dir {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models_for_deploy/United_StatesV4.model.tar.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mextract_and_inspect_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels_for_deploy/United_StatesV4.model.tar.gz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 11\u001b[0m, in \u001b[0;36mextract_and_inspect_model\u001b[0;34m(tar_path, extract_dir)\u001b[0m\n\u001b[1;32m      8\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(extract_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Extract tar.gz\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtarfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtar_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr:gz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m tar:\n\u001b[1;32m     12\u001b[0m     tar\u001b[38;5;241m.\u001b[39mextractall(extract_dir)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Find model directory\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/tarfile.py:1854\u001b[0m, in \u001b[0;36mTarFile.open\u001b[0;34m(cls, name, mode, fileobj, bufsize, **kwargs)\u001b[0m\n\u001b[1;32m   1852\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1853\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CompressionError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown compression type \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m comptype)\n\u001b[0;32m-> 1854\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilemode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1857\u001b[0m     filemode, comptype \u001b[38;5;241m=\u001b[39m mode\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/tarfile.py:1900\u001b[0m, in \u001b[0;36mTarFile.gzopen\u001b[0;34m(cls, name, mode, fileobj, compresslevel, **kwargs)\u001b[0m\n\u001b[1;32m   1897\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CompressionError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip module is not available\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1899\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1900\u001b[0m     fileobj \u001b[38;5;241m=\u001b[39m \u001b[43mGzipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompresslevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1901\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1902\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fileobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/gzip.py:174\u001b[0m, in \u001b[0;36mGzipFile.__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    172\u001b[0m     mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fileobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     fileobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmyfileobj \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fileobj, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models_for_deploy/United_StatesV4.model.tar.gz'"
     ]
    }
   ],
   "source": [
    "extract_and_inspect_model(\"models_for_deploy/United_StatesV4.model.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found SavedModel at: tmp_inspect/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 20:58:58.941632: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-01 20:58:58.945058: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-01 20:58:58.955581: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743541138.974199     669 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743541138.979554     669 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-01 20:58:58.997678: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-01 20:59:00.540659: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['__saved_model_init_op']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['__saved_model_init_op'] tensor_info:\n",
      "        dtype: DT_INVALID\n",
      "        shape: unknown_rank\n",
      "        name: NoOp\n",
      "  Method name is: \n",
      "\n",
      "signature_def['serve']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['numeric_input'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 20)\n",
      "        name: serve_numeric_input:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['output_0'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1)\n",
      "        name: StatefulPartitionedCall:0\n",
      "  Method name is: tensorflow/serving/predict\n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['numeric_input'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 20)\n",
      "        name: serving_default_numeric_input:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['output_0'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1)\n",
      "        name: StatefulPartitionedCall_1:0\n",
      "  Method name is: tensorflow/serving/predict\n",
      "The MetaGraph with tag set ['serve'] contains the following ops: {'AddV2', 'VarHandleOp', 'NoOp', 'RestoreV2', 'RealDiv', 'MatMul', 'Sqrt', 'AssignVariableOp', 'StringJoin', 'Maximum', 'Identity', 'ShardedFilename', 'VarIsInitializedOp', 'SaveV2', 'Sigmoid', 'Const', 'MergeV2Checkpoints', 'Relu', 'BiasAdd', 'StaticRegexFullMatch', 'StatefulPartitionedCall', 'ReadVariableOp', 'DisableCopyOnRead', 'Sub', 'Select', 'Placeholder', 'Pack'}\n",
      "\n",
      "Concrete Functions:\n",
      "  Function Name: 'serve'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          numeric_input: TensorSpec(shape=(None, 20), dtype=tf.float32, name='numeric_input')\n"
     ]
    }
   ],
   "source": [
    "extract_and_inspect_model(\"models_for_deploy/Global.model.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found SavedModel at: tmp_inspect/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-30 21:23:06.159780: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-30 21:23:06.163595: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-30 21:23:06.175357: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743369786.193501     919 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743369786.198752     919 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-30 21:23:06.216573: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-30 21:23:07.705991: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['__saved_model_init_op']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['__saved_model_init_op'] tensor_info:\n",
      "        dtype: DT_INVALID\n",
      "        shape: unknown_rank\n",
      "        name: NoOp\n",
      "  Method name is: \n",
      "\n",
      "signature_def['serve']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['keras_tensor'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 20)\n",
      "        name: serve_keras_tensor:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['output_0'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1)\n",
      "        name: StatefulPartitionedCall:0\n",
      "  Method name is: tensorflow/serving/predict\n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['keras_tensor'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 20)\n",
      "        name: serving_default_keras_tensor:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['output_0'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1)\n",
      "        name: StatefulPartitionedCall_1:0\n",
      "  Method name is: tensorflow/serving/predict\n",
      "The MetaGraph with tag set ['serve'] contains the following ops: {'VarHandleOp', 'SaveV2', 'Sigmoid', 'StaticRegexFullMatch', 'ReadVariableOp', 'ShardedFilename', 'StringJoin', 'Pack', 'DisableCopyOnRead', 'AssignVariableOp', 'Placeholder', 'MatMul', 'AddV2', 'MergeV2Checkpoints', 'BiasAdd', 'Const', 'StatefulPartitionedCall', 'VarIsInitializedOp', 'RestoreV2', 'Select', 'NoOp', 'Identity', 'Relu'}\n",
      "\n",
      "Concrete Functions:\n",
      "  Function Name: 'serve'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          keras_tensor: TensorSpec(shape=(None, 20), dtype=tf.float32, name='keras_tensor')\n"
     ]
    }
   ],
   "source": [
    "extract_and_inspect_model(\"models_for_deploy/ThailandV2.model.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Convert TensorFlow model to a SageMaker readable format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move the TensorFlow exported model into a directory export\\Servo\\. SageMaker will recognize this as a loadable TensorFlow model. Your directory and file structure should look like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Upload All Models to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spain\n",
    "United_Arab_Emirates\n",
    "Canada\n",
    "Kazakhstan\n",
    "Indonesia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "☁️ Uploading Global.model.tar.gz → s3://sagemaker-us-west-2-986030204467/model/Global.model.tar.gz\n",
      "✅ Uploaded to: s3://sagemaker-us-west-2-986030204467/model/Global.model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sagemaker import Session\n",
    "\n",
    "def upload_all_models_to_s3(source_dir=\"models_for_deploy\", s3_prefix=\"model\"):\n",
    "    sess = Session()\n",
    "    bucket = sess.default_bucket()\n",
    "    s3_paths = {}\n",
    "\n",
    "    for filename in os.listdir(source_dir):\n",
    "        if filename.endswith(\".model.tar.gz\"):\n",
    "            local_path = os.path.join(source_dir, filename)\n",
    "            print(f\"☁️ Uploading {filename} → s3://{bucket}/{s3_prefix}/{filename}\")\n",
    "            \n",
    "            s3_uri = sess.upload_data(\n",
    "                path=local_path,\n",
    "                key_prefix=s3_prefix\n",
    "            )\n",
    "            s3_paths[filename] = s3_uri\n",
    "            print(f\"✅ Uploaded to: {s3_uri}\")\n",
    "\n",
    "    return s3_paths\n",
    "    \n",
    "uploaded_model_uris = upload_all_models_to_s3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Global.model.tar.gz': 's3://sagemaker-us-west-2-986030204467/model/Global.model.tar.gz'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uploaded_model_uris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-986030204467/model/Global.model.tar.gz'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uploaded_model_uris['Global.model.tar.gz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 91 μs, sys: 0 ns, total: 91 μs\n",
      "Wall time: 94.4 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#sm_model = Model(model_data=model_data, framework_version=tf_framework_version,role=role)\n",
    "#uncompiled_predictor = sm_model.deploy(initial_instance_count=1, instance_type=instance_type)\n",
    "\n",
    "from sagemaker.tensorflow import TensorFlowModel\n",
    "\n",
    "model = TensorFlowModel(model_data=uploaded_model_uris['Global.model.tar.gz'], role=role, framework_version='2.18.0',\n",
    "                        sagemaker_session=sess)\n",
    "#uncompiled_predictor = model.deploy(initial_instance_count=1, \n",
    "#                                    instance_type=instance_type,image_uri=image_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.multidatamodel import MultiDataModel\n",
    "from time import gmtime, strftime\n",
    "DATA_PREFIX = 'pisa2022'\n",
    "MULTI_MODEL_ARTIFACTS='multi_model_artifacts'\n",
    "ENDPOINT_INSTANCE_TYPE='ml.t2.medium'\n",
    "ENDPOINT_NAME = f'mme-pisa-2022-{strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())}'\n",
    "MODEL_NAME = ENDPOINT_NAME\n",
    "# This is where our MME will read models from on S3.\n",
    "model_data_prefix = f\"s3://{bucket}/{DATA_PREFIX}/{MULTI_MODEL_ARTIFACTS}/\"\n",
    "model_staging_prefix = f\"s3://{bucket}/model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mme = MultiDataModel(\n",
    "    name=MODEL_NAME,\n",
    "    model_data_prefix=model_data_prefix,\n",
    "    model=model,  # passing our model - passes container image needed for the endpoint\n",
    "    sagemaker_session=sess\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Registering Global.model.tar.gz in MME...\n",
      "✅ Registered: Global.model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "def register_models_in_mme(mme, model_s3_dict):\n",
    "    \"\"\"\n",
    "    Given an MME object and a dictionary {filename: s3_uri}, register all models.\n",
    "    \"\"\"\n",
    "    for filename, s3_uri in model_s3_dict.items():\n",
    "        print(f\"📦 Registering {filename} in MME...\")\n",
    "        mme.add_model(\n",
    "            model_data_source=s3_uri,\n",
    "            model_data_path=filename  # This is the TargetModel name\n",
    "        )\n",
    "        print(f\"✅ Registered: {filename}\")\n",
    "\n",
    "register_models_in_mme(mme, uploaded_model_uris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.serving import Model\n",
    "image_uri = '763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-inference:2.18.0-cpu-py310-ubuntu20.04-ec2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "predictor = mme.deploy(\n",
    "    initial_instance_count=1, instance_type=ENDPOINT_INSTANCE_TYPE, endpoint_name=ENDPOINT_NAME, image_uri=image_uri\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'Albania.model.tar.gz',\n",
       " 'Argentina.model.tar.gz',\n",
       " 'Australia.model.tar.gz',\n",
       " 'Austria.model.tar.gz',\n",
       " 'Belgium.model.tar.gz',\n",
       " 'Brazil.model.tar.gz',\n",
       " 'Brunei_Darussalam.model.tar.gz',\n",
       " 'Bulgaria.model.tar.gz',\n",
       " 'Cambodia.model.tar.gz',\n",
       " 'Canada, Mexico, United_States.model.tar.gz',\n",
       " 'Canada.model.tar.gz',\n",
       " 'Chile.model.tar.gz',\n",
       " 'Colombia.model.tar.gz',\n",
       " 'Costa_Rica, El_Salvador, Guatemala, Panama.model.tar.gz',\n",
       " 'Costa_Rica.model.tar.gz',\n",
       " 'Croatia.model.tar.gz',\n",
       " 'Czech_Republic.model.tar.gz',\n",
       " 'Denmark.model.tar.gz',\n",
       " 'Dominican_Republic.model.tar.gz',\n",
       " 'El_Salvador.model.tar.gz',\n",
       " 'Global.model.tar.gz',\n",
       " 'Indonesia.model.tar.gz',\n",
       " 'Japan, Korea, Taiwan.model.tar.gz',\n",
       " 'Kazakhstan.model.tar.gz',\n",
       " 'Latvia.model.tar.gz',\n",
       " 'Lithuania.model.tar.gz',\n",
       " 'Malaysia.model.tar.gz',\n",
       " 'Malta.model.tar.gz',\n",
       " 'Mexico.model.tar.gz',\n",
       " 'Mongolia.model.tar.gz',\n",
       " 'Montenegro.model.tar.gz',\n",
       " 'Morocco.model.tar.gz',\n",
       " 'Netherlands.model.tar.gz',\n",
       " 'New_Zealand.model.tar.gz',\n",
       " 'North_Macedonia.model.tar.gz',\n",
       " 'Norway.model.tar.gz',\n",
       " 'Panama.model.tar.gz',\n",
       " 'Paraguay.model.tar.gz',\n",
       " 'Peru.model.tar.gz',\n",
       " 'Philippines.model.tar.gz',\n",
       " 'Poland.model.tar.gz',\n",
       " 'Portugal.model.tar.gz',\n",
       " 'Qatar.model.tar.gz',\n",
       " 'Republic_of_Moldova.model.tar.gz',\n",
       " 'Romania.model.tar.gz',\n",
       " 'Saudi_Arabia.model.tar.gz',\n",
       " 'Serbia.model.tar.gz',\n",
       " 'Singapore.model.tar.gz',\n",
       " 'Slovak_Republic.model.tar.gz',\n",
       " 'Slovenia.model.tar.gz',\n",
       " 'Spain.model.tar.gz',\n",
       " 'Sweden.model.tar.gz',\n",
       " 'Switzerland.model.tar.gz',\n",
       " 'Taiwan.model.tar.gz',\n",
       " 'Thailand.model.tar.gz',\n",
       " 'ThailandV2.model.tar.gz',\n",
       " 'Türkiye.model.tar.gz',\n",
       " 'United_Arab_Emirates.model.tar.gz',\n",
       " 'United_Kingdom.model.tar.gz',\n",
       " 'United_States.model.tar.gz',\n",
       " 'United_StatesV2.model.tar.gz',\n",
       " 'United_StatesV3.model.tar.gz',\n",
       " 'United_StatesV4.model.tar.gz',\n",
       " 'Uruguay.model.tar.gz',\n",
       " 'Uzbekistan.model.tar.gz',\n",
       " 'Vietnam.model.tar.gz']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(mme.list_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_archive' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msagemaker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01ms3\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m s3_path_join\n\u001b[0;32m----> 2\u001b[0m full_path \u001b[38;5;241m=\u001b[39m s3_path_join(model_staging_prefix, \u001b[43mmodel_archive\u001b[49m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull model path:\u001b[39m\u001b[38;5;124m\"\u001b[39m, full_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_archive' is not defined"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import s3_path_join\n",
    "full_path = s3_path_join(model_staging_prefix, model_archive)\n",
    "print(\"Full model path:\", full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-23 18:05:26     156293 United_States.model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://sagemaker-us-west-2-986030204467/model/United_States.model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "boto3.set_stream_logger('botocore', level='ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-986030204467/pisa2022/multi_model_artifacts/United_States.model.tar.gz'"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mme.add_model(model_data_source=model_staging_prefix+model_archive, model_data_path=model_archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'United_States.model.tar.gz']"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(mme.list_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Deploy the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sagemaker.tensorflow.serving import Model\n",
    "#instance_type = 'ml.c5.xlarge'\n",
    "#image_uri = '763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-inference:2.18.0-cpu-py310-ubuntu20.04-ec2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#sm_model = Model(model_data=model_data, framework_version=tf_framework_version,role=role)\n",
    "#uncompiled_predictor = sm_model.deploy(initial_instance_count=1, instance_type=instance_type)\n",
    "\n",
    "#from sagemaker.tensorflow import TensorFlowModel\n",
    "\n",
    "#model = TensorFlowModel(model_data=model_data, role=role, framework_version='2.18.0')\n",
    "#uncompiled_predictor = model.deploy(initial_instance_count=1, \n",
    "#                                    instance_type=instance_type,image_uri=image_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: Removing leading `/' from member names\n",
      "/\n",
      ".ipynb_checkpoints/\n",
      "1/\n",
      "1/.ipynb_checkpoints/\n",
      "1/assets/\n",
      "1/fingerprint.pb\n",
      "1/saved_model.pb\n",
      "1/variables/\n",
      "1/variables/variables.data-00000-of-00001\n",
      "1/variables/variables.index\n"
     ]
    }
   ],
   "source": [
    "!tar -tzf United_States.model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6. Invoke the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke the SageMaker endpoint from the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 20)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The sample model expects an input of shape [1,50]\n",
    "data = np.random.randn(1, 20)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': 'JSON Value: [[-0.9468745101282406, 0.210982268937443, 0.829047295759814, -0.08553047460908296, 0.35320924351931157, -0.2959219693541786, -1.1321148898031524, -0.4347980390589598, -0.9672111781742968, -0.4698378242986649, 1.7123206100452424, -0.8276450351371874, -0.17478181252612383 Is not object'}"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(data,  initial_args={'TargetModel': 'United_States.model.tar.gz'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize the SageMaker runtime client\n",
    "sagemaker_runtime = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "# Define CSV input (must match your model input shape)\n",
    "csv_input = \"0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0\"  # Example 10-feature row\n",
    "\n",
    "# Invoke the endpoint\n",
    "response = sagemaker_runtime.invoke_endpoint(\n",
    "    EndpointName=\"tensorflow-inference-2025-03-19-03-19-31-180\",\n",
    "    ContentType=\"text/csv\",  # Important: CSV format\n",
    "    Body=csv_input\n",
    ")\n",
    "\n",
    "# Parse the response\n",
    "print(response[\"Body\"].read().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile model using SageMaker Neo\n",
    "\n",
    "[SageMaker Neo](https://aws.amazon.com/sagemaker/neo/) makes it easy to compile pre-trained TensorFlow models and build an inference optimized container without the need for any custom model serving or inference code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_family = 'ml_c5'\n",
    "framework = 'tensorflow'\n",
    "compilation_job_name = 'keras-compile'\n",
    "# output path for compiled model artifact\n",
    "compiled_model_path = 's3://{}/{}/output'.format(bucket, compilation_job_name)\n",
    "data_shape = {'inputs':[1, data.shape[0], data.shape[1]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_estimator = sm_model.compile(target_instance_family=instance_family,\n",
    "                                         input_shape=data_shape,\n",
    "                                         job_name=compilation_job_name,\n",
    "                                         role=role,\n",
    "                                         framework=framework,\n",
    "                                         framework_version=tf_framework_version,\n",
    "                                         output_path=compiled_model_path\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_predictor = optimized_estimator.deploy(initial_instance_count = 1, instance_type = instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke optimized SageMaker endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_predictor.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7. Clean up\n",
    "\n",
    "To avoid incurring charges to your AWS account for the resources used in this tutorial, you need to delete the SageMaker Endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kTSKcyHW-uxg"
   },
   "outputs": [],
   "source": [
    "uncompiled_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this blog post, we demonstrated converting a Keras model to TensorFlow SavedModel format, deploying a trained model to a SageMaker Endpoint, and compiling the same trained model using SageMaker Neo to get better performance. Using Amazon SageMaker, you can take a trained model and in a few lines of code have a scalable, managed inference deployment. This gives you the flexibility to use your existing model training workflows, while easily deploying trained models to production with all the benefits and optimizations offered by a managed platform."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
